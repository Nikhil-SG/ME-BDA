{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Args.py"],"metadata":{"id":"29QWOvxfx9Sj"}},{"cell_type":"code","source":["import argparse\n","\n","parser = argparse.ArgumentParser(description='Speech enhancement,data creation, training and prediction')\n","\n","#mode to run the program (options: data creation, training or prediction)\n","parser.add_argument('--mode',default='prediction', type=str, choices=['data_creation', 'training', 'prediction'])\n","#folders where to find noise audios and clean voice audio to prepare training dataset (mode data_creation)\n","#parser.add_argument('--noise_dir', default='/content/drive/MyDrive/BDA_1_Sem/Mini_Project/Speech-Enhancement/Data/LibriSpeech/Trail.1-Noise', type=str)\n","parser.add_argument('--noise_dir', default='./data/noise', type=str)  # Assuming a \"data\" folder exists within your Colab environment\n","parser.add_argument('--voice_dir', default='./data/voice', type=str)  # Assuming a \"data\" folder exists within your Colab environment\n","#parser.add_argument('--voice_dir', default='/content/drive/MyDrive/BDA_1_Sem/Mini_Project/Speech-Enhancement/Data/LibriSpeech/Trail.1-Train-100-10min', type=str)\n","#folders where to save spectrograms, time series and sounds for training / QC\n","parser.add_argument('--path_save_spectrogram', default='/content/drive/MyDrive/BDA_1_Sem/Mini_Project/Speech-Enhancement/Data/LibriSpeech/spectrogram', type=str)\n","\n","parser.add_argument('--path_save_time_serie', default='/content/drive/MyDrive/BDA_1_Sem/Mini_Project/Speech-Enhancement/Data/LibriSpeech/time_serie', type=str)\n","\n","parser.add_argument('--path_save_sound', default='/content/drive/MyDrive/BDA_1_Sem/Mini_Project/Speech-Enhancement/Data/LibriSpeech/Sound', type=str)\n","#How much frame to create in data_creation mode\n","parser.add_argument('--nb_samples', default=50, type=int)\n","#Training from scratch or pre-trained weights\n","parser.add_argument('--training_from_scratch',default=True, type=bool)\n","#folder of saved weights\n","parser.add_argument('--weights_folder', default='./weights', type=str)\n","#Nb of epochs for training\n","parser.add_argument('--epochs', default=10, type=int)\n","#Batch size for training\n","parser.add_argument('--batch_size', default=20, type=int)\n","#Name of saved model to read\n","parser.add_argument('--name_model', default='model_unet', type=str)\n","#directory where read noisy sound to denoise (prediction mode)\n","parser.add_argument('--audio_dir_prediction', default='./demo_data/test', type=str)\n","#directory to save the denoise sound (prediction mode)\n","parser.add_argument('--dir_save_prediction', default='./demo_data/save_predictions/', type=str)\n","#Noisy sound file to denoise (prediction mode)\n","parser.add_argument('--audio_input_prediction', default=['noisy_voice_long_t2.wav'], type=list)\n","#File name of sound output of denoise prediction\n","parser.add_argument('--audio_output_prediction', default='denoise_t2.wav', type=str)\n","# Sample rate chosen to read audio\n","parser.add_argument('--sample_rate', default=8000, type=int)\n","# Minimum duration of Â  audio files to consider\n","parser.add_argument('--min_duration', default=1.0, type=float)\n","# Training data will be frame of slightly above 1 second\n","parser.add_argument('--frame_length', default=8064, type=int)\n","# hop length for clean voice files separation (no overlap)\n","parser.add_argument('--hop_length_frame', default=8064, type=int)\n","# hop length for noise files to blend (noise is splitted into several windows)\n","parser.add_argument('--hop_length_frame_noise', default=5000, type=int)\n","# Choosing n_fft and hop_length_fft to have squared spectrograms\n","parser.add_argument('--n_fft', default=255, type=int)\n","\n","parser.add_argument('--hop_length_fft', default=63, type=int)\n"],"metadata":{"id":"IhJj6elZx6O6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["data_tools.py"],"metadata":{"id":"sNqjRV9_yAD6"}},{"cell_type":"code","source":["import librosa\n","import numpy as np\n","import os\n","\n","\n","def audio_to_audio_frame_stack(sound_data, frame_length, hop_length_frame):\n","    \"\"\"This function take an audio and split into several frame\n","       in a numpy matrix of size (nb_frame,frame_length)\"\"\"\n","\n","    sequence_sample_length = sound_data.shape[0]\n","\n","    sound_data_list = [sound_data[start:start + frame_length] for start in range(\n","    0, sequence_sample_length - frame_length + 1, hop_length_frame)]  # get sliding windows\n","    sound_data_array = np.vstack(sound_data_list)\n","\n","    return sound_data_array\n","\n","\n","def audio_files_to_numpy(audio_dir, list_audio_files, sample_rate, frame_length, hop_length_frame, min_duration):\n","    \"\"\"This function take audio files of a directory and merge them\n","    in a numpy matrix of size (nb_frame,frame_length) for a sliding window of size hop_length_frame\"\"\"\n","\n","    list_sound_array = []\n","\n","    for file in list_audio_files:\n","        # open the audio file\n","        y, sr = librosa.load(os.path.join(audio_dir, file), sr=sample_rate)\n","        total_duration = librosa.get_duration(y=y, sr=sr)\n","\n","        if (total_duration >= min_duration):\n","            list_sound_array.append(audio_to_audio_frame_stack(\n","                y, frame_length, hop_length_frame))\n","        else:\n","            print(\n","                f\"The following file {os.path.join(audio_dir,file)} is below the min duration\")\n","\n","    return np.vstack(list_sound_array)\n","\n","\n","def blend_noise_randomly(voice, noise, nb_samples, frame_length):\n","    \"\"\"This function takes as input numpy arrays representing frames\n","    of voice sounds, noise sounds and the number of frames to be created\n","    and return numpy arrays with voice randomly blend with noise\"\"\"\n","\n","    prod_voice = np.zeros((nb_samples, frame_length))\n","    prod_noise = np.zeros((nb_samples, frame_length))\n","    prod_noisy_voice = np.zeros((nb_samples, frame_length))\n","\n","    for i in range(nb_samples):\n","        id_voice = np.random.randint(0, voice.shape[0])\n","        id_noise = np.random.randint(0, noise.shape[0])\n","        level_noise = np.random.uniform(0.2, 0.8)\n","        prod_voice[i, :] = voice[id_voice, :]\n","        prod_noise[i, :] = level_noise * noise[id_noise, :]\n","        prod_noisy_voice[i, :] = prod_voice[i, :] + prod_noise[i, :]\n","\n","    return prod_voice, prod_noise, prod_noisy_voice\n","\n","\n","def audio_to_magnitude_db_and_phase(n_fft, hop_length_fft, audio):\n","    \"\"\"This function takes an audio and convert into spectrogram,\n","       it returns the magnitude in dB and the phase\"\"\"\n","\n","    stftaudio = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length_fft)\n","    stftaudio_magnitude, stftaudio_phase = librosa.magphase(stftaudio)\n","\n","    stftaudio_magnitude_db = librosa.amplitude_to_db(\n","        stftaudio_magnitude, ref=np.max)\n","\n","    return stftaudio_magnitude_db, stftaudio_phase\n","\n","\n","def numpy_audio_to_matrix_spectrogram(numpy_audio, dim_square_spec, n_fft, hop_length_fft):\n","    \"\"\"This function takes as input a numpy audi of size (nb_frame,frame_length), and return\n","    a numpy containing the matrix spectrogram for amplitude in dB and phase. It will have the size\n","    (nb_frame,dim_square_spec,dim_square_spec)\"\"\"\n","\n","    nb_audio = numpy_audio.shape[0]\n","\n","    m_mag_db = np.zeros((nb_audio, dim_square_spec, dim_square_spec))\n","    m_phase = np.zeros((nb_audio, dim_square_spec, dim_square_spec), dtype=complex)\n","\n","    for i in range(nb_audio):\n","        m_mag_db[i, :, :], m_phase[i, :, :] = audio_to_magnitude_db_and_phase(\n","            n_fft, hop_length_fft, numpy_audio[i])\n","\n","    return m_mag_db, m_phase\n","\n","\n","def magnitude_db_and_phase_to_audio(frame_length, hop_length_fft, stftaudio_magnitude_db, stftaudio_phase):\n","    \"\"\"This functions reverts a spectrogram to an audio\"\"\"\n","\n","    stftaudio_magnitude_rev = librosa.db_to_amplitude(stftaudio_magnitude_db, ref=1.0)\n","\n","    # taking magnitude and phase of audio\n","    audio_reverse_stft = stftaudio_magnitude_rev * stftaudio_phase\n","    audio_reconstruct = librosa.core.istft(audio_reverse_stft, hop_length=hop_length_fft, length=frame_length)\n","\n","    return audio_reconstruct\n","\n","def matrix_spectrogram_to_numpy_audio(m_mag_db, m_phase, frame_length, hop_length_fft)  :\n","    \"\"\"This functions reverts the matrix spectrograms to numpy audio\"\"\"\n","\n","    list_audio = []\n","\n","    nb_spec = m_mag_db.shape[0]\n","\n","    for i in range(nb_spec):\n","\n","        audio_reconstruct = magnitude_db_and_phase_to_audio(frame_length, hop_length_fft, m_mag_db[i], m_phase[i])\n","        list_audio.append(audio_reconstruct)\n","\n","    return np.vstack(list_audio)\n","\n","def scaled_in(matrix_spec):\n","    \"global scaling apply to noisy voice spectrograms (scale between -1 and 1)\"\n","    matrix_spec = (matrix_spec + 46)/50\n","    return matrix_spec\n","\n","def scaled_ou(matrix_spec):\n","    \"global scaling apply to noise models spectrograms (scale between -1 and 1)\"\n","    matrix_spec = (matrix_spec -6 )/82\n","    return matrix_spec\n","\n","def inv_scaled_in(matrix_spec):\n","    \"inverse global scaling apply to noisy voices spectrograms\"\n","    matrix_spec = matrix_spec * 50 - 46\n","    return matrix_spec\n","\n","def inv_scaled_ou(matrix_spec):\n","    \"inverse global scaling apply to noise models spectrograms\"\n","    matrix_spec = matrix_spec * 82 + 6\n","    return matrix_spec"],"metadata":{"id":"8JPvuos8x6Hv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["data_display.py"],"metadata":{"id":"SqEL5mqoyJZy"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import librosa.display\n","import numpy as np\n","\n","def make_plot_spectrogram(stftaudio_magnitude_db,sample_rate, hop_length_fft) :\n","    \"\"\"This function plots a spectrogram\"\"\"\n","    plt.figure(figsize=(12, 6))\n","    librosa.display.specshow(stftaudio_magnitude_db, x_axis='time', y_axis='linear',\n","                             sr=sample_rate, hop_length=hop_length_fft)\n","    plt.colorbar()\n","    title = 'hop_length={},  time_steps={},  fft_bins={}  (2D resulting shape: {})'\n","    plt.title(title.format(hop_length_fft,\n","                           stftaudio_magnitude_db.shape[1],\n","                           stftaudio_magnitude_db.shape[0],\n","                           stftaudio_magnitude_db.shape));\n","    return\n","\n","def make_plot_phase(stft_phase,sample_rate,hop_length_fft) :\n","    \"\"\"This function plots the phase in radian\"\"\"\n","    plt.figure(figsize=(12, 6))\n","    librosa.display.specshow(np.angle(stft_phase), x_axis='time', y_axis='linear',\n","                             sr=sample_rate, hop_length=hop_length_fft)\n","    plt.colorbar()\n","    title = 'hop_length={},  time_steps={},  fft_bins={}  (2D resulting shape: {})'\n","    plt.title(title.format(hop_length_fft,\n","                           stft_phase.shape[1],\n","                           stft_phase.shape[0],\n","                           stft_phase.shape));\n","    return\n","\n","def make_plot_time_serie(audio,sample_rate):\n","    \"\"\"This function plots the audio as a time serie\"\"\"\n","    plt.figure(figsize=(12, 6))\n","    #plt.ylim(-0.05, 0.05)\n","    plt.title('Audio')\n","    plt.ylabel('Amplitude')\n","    plt.xlabel('Time(s)')\n","    librosa.display.waveplot(audio, sr=sample_rate)\n","    return\n","\n","\n","def make_3plots_spec_voice_noise(stftvoicenoise_mag_db,stftnoise_mag_db,stftvoice_mag_db,sample_rate, hop_length_fft):\n","    \"\"\"This function plots the spectrograms of noisy voice, noise and voice as a single plot \"\"\"\n","    plt.figure(figsize=(8, 12))\n","    plt.subplot(3, 1, 1)\n","    plt.title('Spectrogram voice + noise')\n","    librosa.display.specshow(stftvoicenoise_mag_db, x_axis='time', y_axis='linear',sr=sample_rate, hop_length=hop_length_fft)\n","    plt.colorbar()\n","    plt.subplot(3, 1, 2)\n","    plt.title('Spectrogram predicted voice')\n","    librosa.display.specshow(stftnoise_mag_db, x_axis='time', y_axis='linear',sr=sample_rate, hop_length=hop_length_fft)\n","    plt.colorbar()\n","    plt.subplot(3, 1, 3)\n","    plt.title('Spectrogram true voice')\n","    librosa.display.specshow(stftvoice_mag_db, x_axis='time', y_axis='linear',sr=sample_rate, hop_length=hop_length_fft)\n","    plt.colorbar()\n","    plt.tight_layout()\n","\n","    return\n","\n","\n","def make_3plots_phase_voice_noise(stftvoicenoise_phase,stftnoise_phase,stftvoice_phase,sample_rate, hop_length_fft):\n","    \"\"\"This function plots the phase in radians of noisy voice, noise and voice as a single plot \"\"\"\n","    plt.figure(figsize=(8, 12))\n","    plt.subplot(3, 1, 1)\n","    plt.title('Phase (radian) voice + noise')\n","    librosa.display.specshow(np.angle(stftvoicenoise_phase), x_axis='time', y_axis='linear',sr=sample_rate, hop_length=hop_length_fft)\n","    plt.colorbar()\n","    plt.subplot(3, 1, 2)\n","    plt.title('Phase (radian) predicted voice')\n","    librosa.display.specshow(np.angle(stftnoise_phase), x_axis='time', y_axis='linear',sr=sample_rate, hop_length=hop_length_fft)\n","    plt.colorbar()\n","    plt.subplot(3, 1, 3)\n","    plt.title('Phase (radian) true voice')\n","    librosa.display.specshow(np.angle(stftvoice_phase), x_axis='time', y_axis='linear',sr=sample_rate, hop_length=hop_length_fft)\n","    plt.colorbar()\n","    plt.tight_layout()\n","\n","    return\n","\n","\n","def make_3plots_timeseries_voice_noise(clipvoicenoise,clipnoise,clipvoice, sample_rate) :\n","    \"\"\"This function plots the time series of audio of noisy voice, noise and voice as a single plot \"\"\"\n","    #y_ax_min = min(clipvoicenoise) - 0.15\n","    #y_ax_max = max(clipvoicenoise) + 0.15\n","\n","    plt.figure(figsize=(18, 12))\n","    plt.subplots_adjust(hspace=0.35)\n","    plt.subplot(3, 1, 1)\n","    plt.title('Audio voice + noise')\n","    plt.ylabel('Amplitude')\n","    plt.xlabel('Time(s)')\n","    librosa.display.waveplot(clipvoicenoise, sr=sample_rate)\n","    plt.ylim(-0.05, 0.05)\n","    plt.subplot(3, 1, 2)\n","    plt.title('Audio predicted voice')\n","    plt.ylabel('Amplitude')\n","    plt.xlabel('Time(s)')\n","    librosa.display.waveplot(clipnoise, sr=sample_rate)\n","    plt.ylim(-0.05, 0.05)\n","    plt.subplot(3, 1, 3)\n","    plt.title('Audio true voice')\n","    plt.ylabel('Amplitude')\n","    plt.xlabel('Time(s)')\n","    librosa.display.waveplot(clipvoice, sr=sample_rate)\n","    plt.ylim(-0.05, 0.05)\n","\n","    return"],"metadata":{"id":"LFxQQlcwx5-i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["prepare_data.py"],"metadata":{"id":"bwROaJK-2P0Q"}},{"cell_type":"code","source":["import os\n","import librosa\n","from data_tools import audio_files_to_numpy\n","from data_tools import blend_noise_randomly, numpy_audio_to_matrix_spectrogram\n","import numpy as np\n","\n","\n","\n","def create_data(noise_dir, voice_dir, path_save_time_serie, path_save_sound, path_save_spectrogram, sample_rate,\n","min_duration, frame_length, hop_length_frame, hop_length_frame_noise, nb_samples, n_fft, hop_length_fft):\n","    \"\"\"This function will randomly blend some clean voices from voice_dir with some noises from noise_dir\n","    and save the spectrograms of noisy voice, noise and clean voices to disk as well as complex phase,\n","    time series and sounds. This aims at preparing datasets for denoising training. It takes as inputs\n","    parameters defined in args module\"\"\"\n","\n","    list_noise_files = os.listdir(noise_dir)\n","    list_voice_files = os.listdir(voice_dir)\n","\n","    def remove_ds_store(lst):\n","        \"\"\"remove mac specific file if present\"\"\"\n","        if '.DS_Store' in lst:\n","            lst.remove('.DS_Store')\n","\n","        return lst\n","\n","    list_noise_files = remove_ds_store(list_noise_files)\n","    list_voice_files = remove_ds_store(list_voice_files)\n","\n","    nb_voice_files = len(list_voice_files)\n","    nb_noise_files = len(list_noise_files)\n","\n","\n","    # Extracting noise and voice from folder and convert to numpy\n","    noise = audio_files_to_numpy(noise_dir, list_noise_files, sample_rate,\n","                                     frame_length, hop_length_frame_noise, min_duration)\n","\n","    voice = audio_files_to_numpy(voice_dir, list_voice_files,\n","                                     sample_rate, frame_length, hop_length_frame, min_duration)\n","\n","    # Blend some clean voices with random selected noises (and a random level of noise)\n","    prod_voice, prod_noise, prod_noisy_voice = blend_noise_randomly(\n","            voice, noise, nb_samples, frame_length)\n","\n","    # To save the long audio generated to disk to QC:\n","    noisy_voice_long = prod_noisy_voice.reshape(1, nb_samples * frame_length)\n","    librosa.output.write_wav(path_save_sound + 'noisy_voice_long.wav', noisy_voice_long[0, :], sample_rate)\n","    voice_long = prod_voice.reshape(1, nb_samples * frame_length)\n","    librosa.output.write_wav(path_save_sound + 'voice_long.wav', voice_long[0, :], sample_rate)\n","    noise_long = prod_noise.reshape(1, nb_samples * frame_length)\n","    librosa.output.write_wav(path_save_sound + 'noise_long.wav', noise_long[0, :], sample_rate)\n","\n","    # Squared spectrogram dimensions\n","    dim_square_spec = int(n_fft / 2) + 1\n","\n","    # Create Amplitude and phase of the sounds\n","    m_amp_db_voice,  m_pha_voice = numpy_audio_to_matrix_spectrogram(\n","            prod_voice, dim_square_spec, n_fft, hop_length_fft)\n","    m_amp_db_noise,  m_pha_noise = numpy_audio_to_matrix_spectrogram(\n","            prod_noise, dim_square_spec, n_fft, hop_length_fft)\n","    m_amp_db_noisy_voice,  m_pha_noisy_voice = numpy_audio_to_matrix_spectrogram(\n","            prod_noisy_voice, dim_square_spec, n_fft, hop_length_fft)\n","\n","    # Save to disk for Training / QC\n","    np.save(path_save_time_serie + 'voice_timeserie', prod_voice)\n","    np.save(path_save_time_serie + 'noise_timeserie', prod_noise)\n","    np.save(path_save_time_serie + 'noisy_voice_timeserie', prod_noisy_voice)\n","\n","\n","    np.save(path_save_spectrogram + 'voice_amp_db', m_amp_db_voice)\n","    np.save(path_save_spectrogram + 'noise_amp_db', m_amp_db_noise)\n","    np.save(path_save_spectrogram + 'noisy_voice_amp_db', m_amp_db_noisy_voice)\n","\n","    np.save(path_save_spectrogram + 'voice_pha_db', m_pha_voice)\n","    np.save(path_save_spectrogram + 'noise_pha_db', m_pha_noise)\n","    np.save(path_save_spectrogram + 'noisy_voice_pha_db', m_pha_noisy_voice)"],"metadata":{"id":"zaZoy95t2J8S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from args import parser\n","import os\n","from prepare_data import create_data\n","from train_model import training\n","from prediction_denoise import prediction\n","\n","if __name__ == '__main__':\n","\n","    args = parser.parse_args()\n","\n","    mode = args.mode\n","\n","    # Initialize all modes to zero\n","    data_mode = False\n","    training_mode = False\n","    prediction_mode = False\n","\n","    # Update with the mode the user is asking\n","    if mode == 'prediction':\n","        prediction_mode = True\n","    elif mode == 'training':\n","        training_mode = True\n","    elif mode == 'data_creation':\n","        data_mode = True\n","\n","    if data_mode:\n","        #Example: python main.py --mode='data_creation'\n","\n","        #folder containing noises\n","        noise_dir = args.noise_dir\n","        #folder containing clean voices\n","        voice_dir = args.voice_dir\n","        #path to save time series\n","        path_save_time_serie = args.path_save_time_serie\n","        #path to save sounds\n","        path_save_sound = args.path_save_sound\n","        #path to save spectrograms\n","        path_save_spectrogram = args.path_save_spectrogram\n","        # Sample rate to read audio\n","        sample_rate = args.sample_rate\n","        # Minimum duration of audio files to consider\n","        min_duration = args.min_duration\n","        #Frame length for training data\n","        frame_length = args.frame_length\n","        # hop length for clean voice files\n","        hop_length_frame = args.hop_length_frame\n","        # hop length for noise files\n","        hop_length_frame_noise = args.hop_length_frame_noise\n","        # How much frame to create for training\n","        nb_samples = args.nb_samples\n","        #nb of points for fft(for spectrogram computation)\n","        n_fft = args.n_fft\n","        #hop length for fft\n","        hop_length_fft = args.hop_length_fft\n","\n","        create_data(noise_dir, voice_dir, path_save_time_serie, path_save_sound, path_save_spectrogram, sample_rate,\n","        min_duration, frame_length, hop_length_frame, hop_length_frame_noise, nb_samples, n_fft, hop_length_fft)\n","\n","\n","    elif training_mode:\n","        #Example: python main.py --mode=\"training\"\n","        #Path were to read spectrograms of noisy voice and clean voice\n","        path_save_spectrogram = args.path_save_spectrogram\n","        #path to find pre-trained weights / save models\n","        weights_path = args.weights_folder\n","        #pre trained model\n","        name_model = args.name_model\n","        #Training from scratch vs training from pre-trained weights\n","        training_from_scratch = args.training_from_scratch\n","        #epochs for training\n","        epochs = args.epochs\n","        #batch size for training\n","        batch_size = args.batch_size\n","\n","        training(path_save_spectrogram, weights_path, name_model, training_from_scratch, epochs, batch_size)\n","\n","    elif prediction_mode:\n","        #Example: python main.py --mode=\"prediction\"\n","        #path to find pre-trained weights / save models\n","        weights_path = args.weights_folder\n","        #pre trained model\n","        name_model = args.name_model\n","        #directory where read noisy sound to denoise\n","        audio_dir_prediction = args.audio_dir_prediction\n","        #directory to save the denoise sound\n","        dir_save_prediction = args.dir_save_prediction\n","        #Name noisy sound file to denoise\n","        audio_input_prediction = args.audio_input_prediction\n","        #Name of denoised sound file to save\n","        audio_output_prediction = args.audio_output_prediction\n","        # Sample rate to read audio\n","        sample_rate = args.sample_rate\n","        # Minimum duration of audio files to consider\n","        min_duration = args.min_duration\n","        #Frame length for training data\n","        frame_length = args.frame_length\n","        # hop length for sound files\n","        hop_length_frame = args.hop_length_frame\n","        #nb of points for fft(for spectrogram computation)\n","        n_fft = args.n_fft\n","        #hop length for fft\n","        hop_length_fft = args.hop_length_fft\n","\n","        prediction(weights_path, name_model, audio_dir_prediction, dir_save_prediction, audio_input_prediction,\n","        audio_output_prediction, sample_rate, min_duration, frame_length, hop_length_frame, n_fft, hop_length_fft)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":473},"collapsed":true,"id":"33fSNpd4x3c7","executionInfo":{"status":"error","timestamp":1723989576587,"user_tz":-330,"elapsed":4908,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"0ef10638-d528-4241-916c-f1206e59e430"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.17.0\n"]},{"output_type":"stream","name":"stderr","text":["usage: colab_kernel_launcher.py [-h] [--mode {data_creation,training,prediction}]\n","                                [--noise_dir NOISE_DIR] [--voice_dir VOICE_DIR]\n","                                [--path_save_spectrogram PATH_SAVE_SPECTROGRAM]\n","                                [--path_save_time_serie PATH_SAVE_TIME_SERIE]\n","                                [--path_save_sound PATH_SAVE_SOUND] [--nb_samples NB_SAMPLES]\n","                                [--training_from_scratch TRAINING_FROM_SCRATCH]\n","                                [--weights_folder WEIGHTS_FOLDER] [--epochs EPOCHS]\n","                                [--batch_size BATCH_SIZE] [--name_model NAME_MODEL]\n","                                [--audio_dir_prediction AUDIO_DIR_PREDICTION]\n","                                [--dir_save_prediction DIR_SAVE_PREDICTION]\n","                                [--audio_input_prediction AUDIO_INPUT_PREDICTION]\n","                                [--audio_output_prediction AUDIO_OUTPUT_PREDICTION]\n","                                [--sample_rate SAMPLE_RATE] [--min_duration MIN_DURATION]\n","                                [--frame_length FRAME_LENGTH]\n","                                [--hop_length_frame HOP_LENGTH_FRAME]\n","                                [--hop_length_frame_noise HOP_LENGTH_FRAME_NOISE] [--n_fft N_FFT]\n","                                [--hop_length_fft HOP_LENGTH_FFT]\n","colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-62eff921-3eff-47ba-8611-4917e62ba9da.json\n"]},{"output_type":"error","ename":"SystemExit","evalue":"2","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}]},{"cell_type":"code","source":["!python main.py --mode data_creation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HrhuP48MyrdL","executionInfo":{"status":"ok","timestamp":1724073196082,"user_tz":-330,"elapsed":1590,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"4de3ee85-2133-48cc-c4de-429757a30210"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["python3: can't open file '/content/main.py': [Errno 2] No such file or directory\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MePwbtUPnQQU","executionInfo":{"status":"ok","timestamp":1724070838106,"user_tz":-330,"elapsed":23055,"user":{"displayName":"Vinayashree M shet","userId":"16359732287653994305"}},"outputId":"6c496d65-7f1c-4b33-9ac9-ef6a2ec886e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!python main.py --mode data_creation --noise_dir /content/drive/MyDrive/BDA_1_Sem/Mini_Project/Speech-Enhancement/Data/LibriSpeech/Trail.1-Noise --voice_dir /content/drive/MyDrive/BDA_1_Sem/Mini_Project/Speech-Enhancement/Data/LibriSpeech/Trail.1-Train-100-10min"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rb1cw2Fs3SuL","executionInfo":{"status":"ok","timestamp":1724070851676,"user_tz":-330,"elapsed":1449,"user":{"displayName":"Vinayashree M shet","userId":"16359732287653994305"}},"outputId":"f9043537-c6c8-4754-b782-da1d131d96f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["python3: can't open file '/content/main.py': [Errno 2] No such file or directory\n"]}]},{"cell_type":"code","source":["!pip install tensorflow-gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d_GVHbs_zKm-","executionInfo":{"status":"ok","timestamp":1724070751118,"user_tz":-330,"elapsed":2396,"user":{"displayName":"Vinayashree M shet","userId":"16359732287653994305"}},"outputId":"eaa310c4-0f6f-4fb4-ff99-97e0706e1879"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow-gpu\n","  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31mÃ\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31mâ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31mâ°â>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31mÃ\u001b[0m Encountered error while generating package metadata.\n","\u001b[31mâ°â>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n"]}]},{"cell_type":"code","source":["!pip uninstall tensorflow-gpu -y  # Uninstall any existing version (optional)\n","!pip install tensorflow-gpu==2.11.0  # Specify a compatible version\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"fanY9OnkzSKI","executionInfo":{"status":"ok","timestamp":1723989901633,"user_tz":-330,"elapsed":69089,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"0b1bc6a0-8fae-4e90-a9d4-6d11fc32c211"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mCollecting tensorflow-gpu==2.11.0\n","  Downloading tensorflow_gpu-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.11.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.11.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.11.0) (24.3.25)\n","Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-gpu==2.11.0)\n","  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.11.0) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.11.0) (1.64.1)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.11.0) (3.11.0)\n","Collecting keras<2.12,>=2.11.0 (from tensorflow-gpu==2.11.0)\n","  Downloading keras-2.11.0-py2.py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.11.0) (18.1.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.11.0) (1.26.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.11.0) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.11.0) (24.1)\n","Collecting protobuf<3.20,>=3.9.2 (from tensorflow-gpu==2.11.0)\n","  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.11.0) (71.0.4)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.11.0) (1.16.0)\n","Collecting tensorboard<2.12,>=2.11 (from tensorflow-gpu==2.11.0)\n","  Downloading tensorboard-2.11.2-py3-none-any.whl.metadata (1.9 kB)\n","Collecting tensorflow-estimator<2.12,>=2.11.0 (from tensorflow-gpu==2.11.0)\n","  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.11.0) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.11.0) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.11.0) (1.16.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.11.0) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow-gpu==2.11.0) (0.44.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu==2.11.0) (2.27.0)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.12,>=2.11->tensorflow-gpu==2.11.0)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu==2.11.0) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu==2.11.0) (2.32.3)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.12,>=2.11->tensorflow-gpu==2.11.0)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n","Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.12,>=2.11->tensorflow-gpu==2.11.0)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu==2.11.0) (3.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu==2.11.0) (5.4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu==2.11.0) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu==2.11.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-gpu==2.11.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu==2.11.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu==2.11.0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu==2.11.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu==2.11.0) (2024.7.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-gpu==2.11.0) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu==2.11.0) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-gpu==2.11.0) (3.2.2)\n","Downloading tensorflow_gpu-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n","\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n","\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n","\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, tensorflow-estimator, tensorboard-data-server, protobuf, keras, gast, google-auth-oauthlib, tensorboard, tensorflow-gpu\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.2\n","    Uninstalling tensorboard-data-server-0.7.2:\n","      Successfully uninstalled tensorboard-data-server-0.7.2\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","  Attempting uninstall: keras\n","    Found existing installation: keras 3.4.1\n","    Uninstalling keras-3.4.1:\n","      Successfully uninstalled keras-3.4.1\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.6.0\n","    Uninstalling gast-0.6.0:\n","      Successfully uninstalled gast-0.6.0\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.2.1\n","    Uninstalling google-auth-oauthlib-1.2.1:\n","      Successfully uninstalled google-auth-oauthlib-1.2.1\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.17.0\n","    Uninstalling tensorboard-2.17.0:\n","      Successfully uninstalled tensorboard-2.17.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-bigquery-connection 1.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-bigtable 2.25.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-functions 1.16.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-iam 2.15.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-language 2.13.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-pubsub 2.23.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-resource-manager 1.12.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-translate 3.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","googleapis-common-protos 1.63.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","pandas-gbq 0.23.1 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n","tensorflow 2.17.0 requires keras>=3.2.0, but you have keras 2.11.0 which is incompatible.\n","tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow 2.17.0 requires tensorboard<2.18,>=2.17, but you have tensorboard 2.11.2 which is incompatible.\n","tensorflow-datasets 4.9.6 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.11.0 protobuf-3.19.6 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-estimator-2.11.0 tensorflow-gpu-2.11.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gast","google","keras","tensorflow"]},"id":"f1dadc25e5da4743a17c7285d2542431"}},"metadata":{}}]},{"cell_type":"code","source":["import numpy as np\n","img_array = np.load('/content/noisy_voice_amp_db.npy')\n","for i in range(len(img_array)):\n","    image = img_array[i]  # Access each image in the batch\n","    plt.imshow(image, cmap='gray')  # Assuming grayscale images\n","    plt.title(f\"Image {i+1}\")  # Optional: Add title for each image\n","    plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1OqySLDefCtddK6ie6x6JZILTW3PsVD_S"},"collapsed":true,"id":"9Q1e1AM37aR5","executionInfo":{"status":"ok","timestamp":1723992224563,"user_tz":-330,"elapsed":21307,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"80b4db48-1238-4202-9bcd-ff7f32bf4485"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as colors\n","\n","def create_purple_black_cmap():\n","  # Create a colormap with purple and dark black\n","  color_list = ['purple', 'black']\n","  cmap = colors.LinearSegmentedColormap.from_list('purple_black', color_list)\n","  return cmap\n","\n","# Load your image data (replace with your actual data loading)\n","img_array = np.load('/content/noisy_voice_amp_db.npy')\n","\n","# Create the colormap\n","cmap = create_purple_black_cmap()\n","\n","# Display images\n","for i in range(len(img_array)):\n","  image = img_array[i]\n","  plt.imshow(image, cmap=cmap)\n","  plt.title(f\"Image {i+1}\")\n","  plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"10JtP67DCr_lMSfs3eLNHBeT9kSk9PB3x"},"id":"UN7ObD9e8n77","executionInfo":{"status":"ok","timestamp":1723992300810,"user_tz":-330,"elapsed":20155,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"f2aa52d2-1009-481e-b8e1-83d795db7ff7"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}