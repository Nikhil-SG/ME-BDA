{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPh/m8u0iVmV3sH2DgdxYil"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##                              RDD assignments  "],"metadata":{"id":"5DWSD0VJ401o"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LXPc1nSpo7qc","executionInfo":{"status":"ok","timestamp":1728105684004,"user_tz":-330,"elapsed":25106,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"f8ae0b64-09db-4278-d67b-d8422b9192de"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X0LTyaT0nIPe","executionInfo":{"status":"ok","timestamp":1728105768007,"user_tz":-330,"elapsed":44637,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"885cc572-47f2-40ca-a8a1-b96f43f6910a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=84fdaf38998aefd7f33416f6c37af33641564f99b5f5b633089b9a763c9182ba\n","  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.3\n"]}]},{"cell_type":"code","execution_count":77,"metadata":{"id":"AlPkgHjBmMCb","executionInfo":{"status":"ok","timestamp":1728112997683,"user_tz":-330,"elapsed":451,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}}},"outputs":[],"source":["import pyspark\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName('RDD').getOrCreate()\n","spark1 = SparkSession.builder.appName('RDD1').getOrCreate()"]},{"cell_type":"code","source":["type(spark)\n","type(spark1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"eOcWjsH5nEw2","executionInfo":{"status":"ok","timestamp":1728113005269,"user_tz":-330,"elapsed":455,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"f33098b0-e63e-4829-dc78-0f2fd2de8d52"},"execution_count":78,"outputs":[{"output_type":"execute_result","data":{"text/plain":["pyspark.sql.session.SparkSession"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.session.SparkSession</b><br/>def __init__(sparkContext: SparkContext, jsparkSession: Optional[JavaObject]=None, options: Dict[str, Any]={})</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py</a>The entry point to programming Spark with the Dataset and DataFrame API.\n","\n","A SparkSession can be used to create :class:`DataFrame`, register :class:`DataFrame` as\n","tables, execute SQL over tables, cache tables, and read parquet files.\n","To create a :class:`SparkSession`, use the following builder pattern:\n","\n",".. versionchanged:: 3.4.0\n","    Supports Spark Connect.\n","\n",".. autoattribute:: builder\n","   :annotation:\n","\n","Examples\n","--------\n","Create a Spark session.\n","\n","&gt;&gt;&gt; spark = (\n","...     SparkSession.builder\n","...         .master(&quot;local&quot;)\n","...         .appName(&quot;Word Count&quot;)\n","...         .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)\n","...         .getOrCreate()\n","... )\n","\n","Create a Spark session with Spark Connect.\n","\n","&gt;&gt;&gt; spark = (\n","...     SparkSession.builder\n","...         .remote(&quot;sc://localhost&quot;)\n","...         .appName(&quot;Word Count&quot;)\n","...         .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)\n","...         .getOrCreate()\n","... )  # doctest: +SKIP</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 166);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":78}]},{"cell_type":"markdown","source":["Q1) Create RDDs in three different ways.\n"],"metadata":{"id":"Xsk7fnnWqGBq"}},{"cell_type":"markdown","source":["Creating RDD using parallized method"],"metadata":{"id":"jdq46dHaqIqE"}},{"cell_type":"code","source":["A11_1 = spark.sparkContext.parallelize([1,2,3,4,5])\n","A11_1.collect()\n","A11_1.count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BmU-LzPlqThd","executionInfo":{"status":"ok","timestamp":1728106704635,"user_tz":-330,"elapsed":882,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"4030c9c5-e765-4acc-f22d-5af06f493bca"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["Creating RDD using Data Source method"],"metadata":{"id":"Fl6BwbmJ3AgX"}},{"cell_type":"code","source":["A11_2=spark.sparkContext.textFile('/content/drive/MyDrive/BDA_1_Sem/ABD/ABD_Lab/Hadoop/RDD Lab assignments/Input.txt')\n","A11_2.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZTYVevGerCKr","executionInfo":{"status":"ok","timestamp":1728107512324,"user_tz":-330,"elapsed":926,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"7852b2d4-af04-4ba6-b1fe-a1baea8c85e1"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello world',\n"," 'This is a sample text file.',\n"," 'It contains a few words for testing.',\n"," 'Apache Spark is great for big data processing.',\n"," 'Have fun with RDDs!']"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["Creating Using Transform Method"],"metadata":{"id":"nGJA983e3fPu"}},{"cell_type":"code","source":["A11_3 = A11_1.map(lambda x: x ** 2)\n","print(\"Transformed RDD (squared values):\", A11_3.collect())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qPw-WiJN3b2B","executionInfo":{"status":"ok","timestamp":1728107516717,"user_tz":-330,"elapsed":423,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"ef80816e-a455-4e6c-c910-aa537c5d7a3f"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Transformed RDD (squared values): [1, 4, 9, 16, 25]\n"]}]},{"cell_type":"markdown","source":["Q2) Read a text file and count the number of words in the file using RDD operation"],"metadata":{"id":"PdD6-YfX3tpw"}},{"cell_type":"code","source":["A11_2.flatMap(lambda x:x.split(' ')).count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7nsBgZx3uAH","executionInfo":{"status":"ok","timestamp":1728107520082,"user_tz":-330,"elapsed":915,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"799acfe6-09dd-4f24-a073-3576736fe82c"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["27"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["Q3) Write a program to find the word frequency in a given file"],"metadata":{"id":"a2GvObk637Xn"}},{"cell_type":"code","source":["wordRDD=A11_2.flatMap(lambda x:x.split(' '))\n","wordRDD.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X07ONySv306m","executionInfo":{"status":"ok","timestamp":1728107639542,"user_tz":-330,"elapsed":434,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"a79d0a6f-c0b6-49bd-cee7-6f13abeea19b"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello',\n"," 'world',\n"," 'This',\n"," 'is',\n"," 'a',\n"," 'sample',\n"," 'text',\n"," 'file.',\n"," 'It',\n"," 'contains',\n"," 'a',\n"," 'few',\n"," 'words',\n"," 'for',\n"," 'testing.',\n"," 'Apache',\n"," 'Spark',\n"," 'is',\n"," 'great',\n"," 'for',\n"," 'big',\n"," 'data',\n"," 'processing.',\n"," 'Have',\n"," 'fun',\n"," 'with',\n"," 'RDDs!']"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["wordcount=wordRDD.map(lambda x:(x,1))\n","wordcount.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iFjE2tq8PQQi","executionInfo":{"status":"ok","timestamp":1728107630430,"user_tz":-330,"elapsed":445,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"f5fc7cbd-e0ff-437d-f393-2be5a6bae92e"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Hello', 1),\n"," ('world', 1),\n"," ('This', 1),\n"," ('is', 1),\n"," ('a', 1),\n"," ('sample', 1),\n"," ('text', 1),\n"," ('file.', 1),\n"," ('It', 1),\n"," ('contains', 1),\n"," ('a', 1),\n"," ('few', 1),\n"," ('words', 1),\n"," ('for', 1),\n"," ('testing.', 1),\n"," ('Apache', 1),\n"," ('Spark', 1),\n"," ('is', 1),\n"," ('great', 1),\n"," ('for', 1),\n"," ('big', 1),\n"," ('data', 1),\n"," ('processing.', 1),\n"," ('Have', 1),\n"," ('fun', 1),\n"," ('with', 1),\n"," ('RDDs!', 1)]"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["wordcount.reduceByKey(lambda x,y:x+y).collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWylDYCfPSPZ","executionInfo":{"status":"ok","timestamp":1728107590354,"user_tz":-330,"elapsed":1325,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"ebd4ee2a-959f-46f9-ad32-7b21def61413"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Hello', 1),\n"," ('world', 1),\n"," ('is', 2),\n"," ('file.', 1),\n"," ('It', 1),\n"," ('Apache', 1),\n"," ('Spark', 1),\n"," ('RDDs!', 1),\n"," ('This', 1),\n"," ('a', 2),\n"," ('sample', 1),\n"," ('text', 1),\n"," ('contains', 1),\n"," ('few', 1),\n"," ('words', 1),\n"," ('for', 2),\n"," ('testing.', 1),\n"," ('great', 1),\n"," ('big', 1),\n"," ('data', 1),\n"," ('processing.', 1),\n"," ('Have', 1),\n"," ('fun', 1),\n"," ('with', 1)]"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["Q4) Write a program to convert all words in a file to uppercase."],"metadata":{"id":"r5BWlPGx5JTw"}},{"cell_type":"code","source":["wordRDD=A11_2.flatMap(lambda x:x.split(' '))\n","upperWordsRDD = wordRDD.map(lambda word: word.upper())\n","result = upperWordsRDD.collect()\n","for word in result:\n","    print(word)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2QTGU0PT5P9Y","executionInfo":{"status":"ok","timestamp":1728107699794,"user_tz":-330,"elapsed":891,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"c184ed97-be0b-46b5-9b86-a6c0d59c6de7"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["HELLO\n","WORLD\n","THIS\n","IS\n","A\n","SAMPLE\n","TEXT\n","FILE.\n","IT\n","CONTAINS\n","A\n","FEW\n","WORDS\n","FOR\n","TESTING.\n","APACHE\n","SPARK\n","IS\n","GREAT\n","FOR\n","BIG\n","DATA\n","PROCESSING.\n","HAVE\n","FUN\n","WITH\n","RDDS!\n"]}]},{"cell_type":"markdown","source":["Q5) Write a program to convert all words in a file to lowercase."],"metadata":{"id":"6S98FIhCQsWx"}},{"cell_type":"code","source":["wordRDD = A11_2.flatMap(lambda x:x.split())\n","toLower = wordRDD.map(lambda x:x.lower())\n","result = toLower.collect()\n","for word in result:\n","    print(word)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GWHBzg6YPvAw","executionInfo":{"status":"ok","timestamp":1728110925264,"user_tz":-330,"elapsed":875,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"ddca6024-007a-4069-fb8f-569235b7bc21"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["hello\n","world\n","this\n","is\n","a\n","sample\n","text\n","file.\n","it\n","contains\n","a\n","few\n","words\n","for\n","testing.\n","apache\n","spark\n","is\n","great\n","for\n","big\n","data\n","processing.\n","have\n","fun\n","with\n","rdds!\n"]}]},{"cell_type":"markdown","source":["Q6) Write a program to capitalize first letter of each words in file (use string capitalize()\n","method)."],"metadata":{"id":"9BYYYTVTcCLO"}},{"cell_type":"code","source":["wordRdd = A11_2.flatMap(lambda x:x.split())\n","cap = wordRdd.map(lambda x: x.capitalize())\n","result = cap.collect()\n","for word in result:\n","    print(word)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y0oxBODQcFHh","executionInfo":{"status":"ok","timestamp":1728111013335,"user_tz":-330,"elapsed":855,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"a0cb2038-ad81-4634-ab8d-be36ddbdf6b7"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello\n","World\n","This\n","Is\n","A\n","Sample\n","Text\n","File.\n","It\n","Contains\n","A\n","Few\n","Words\n","For\n","Testing.\n","Apache\n","Spark\n","Is\n","Great\n","For\n","Big\n","Data\n","Processing.\n","Have\n","Fun\n","With\n","Rdds!\n"]}]},{"cell_type":"markdown","source":["Q7) Find the number of occurrence of a word in a given file."],"metadata":{"id":"hiUpfOrfcYCO"}},{"cell_type":"code","source":["wordRDD=A11_2.flatMap(lambda x:x.split(' '))\n","countRDD = wordRDD.map(lambda x:(x,1))\n","countRDD.reduceByKey(lambda x,y:x+y).collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZHor89Q0cXqf","executionInfo":{"status":"ok","timestamp":1728111725647,"user_tz":-330,"elapsed":1950,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"36ee3ffc-711c-4348-f459-7e904d7f7bd2"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Hello', 1),\n"," ('world', 1),\n"," ('is', 2),\n"," ('file.', 1),\n"," ('It', 1),\n"," ('Apache', 1),\n"," ('Spark', 1),\n"," ('RDDs!', 1),\n"," ('This', 1),\n"," ('a', 2),\n"," ('sample', 1),\n"," ('text', 1),\n"," ('contains', 1),\n"," ('few', 1),\n"," ('words', 1),\n"," ('for', 2),\n"," ('testing.', 1),\n"," ('great', 1),\n"," ('big', 1),\n"," ('data', 1),\n"," ('processing.', 1),\n"," ('Have', 1),\n"," ('fun', 1),\n"," ('with', 1)]"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["word_to_count='is'\n","count=countRDD.filter(lambda word: word[0] == word_to_count).count()\n","print(f\"The word '{word_to_count}' appears {count} times.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mdUPPjZfer8Z","executionInfo":{"status":"ok","timestamp":1728111735585,"user_tz":-330,"elapsed":438,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"9552ba74-758b-462d-fff3-2ea24f3d8efe"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["The word 'is' appears 2 times.\n"]}]},{"cell_type":"markdown","source":["Q8) Select only the sentences containing given word from a text file."],"metadata":{"id":"VI6c4UFWfN6Y"}},{"cell_type":"code","source":["wordRDD=A11_2.flatMap(lambda x:x.split(','))\n","wordRDD.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EDYFKERbfJHF","executionInfo":{"status":"ok","timestamp":1728111824292,"user_tz":-330,"elapsed":880,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"8bd9c476-ada6-4a35-df7e-1ade0c073a5f"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello world',\n"," 'This is a sample text file.',\n"," 'It contains a few words for testing.',\n"," 'Apache Spark is great for big data processing.',\n"," 'Have fun with RDDs!']"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["word = 'is'\n","filteredRDD = wordRDD.filter(lambda x: word in x)\n","filteredRDD.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"acPvI7ABfdyd","executionInfo":{"status":"ok","timestamp":1728111849319,"user_tz":-330,"elapsed":437,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"62f686a8-3216-4d27-88dd-3e9bb5715b79"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['This is a sample text file.',\n"," 'Apache Spark is great for big data processing.']"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","source":["Q9) Find the longest length of word from given set of words."],"metadata":{"id":"FtRA8CvofnjJ"}},{"cell_type":"code","source":["# First RDD for words\n","Words = spark.sparkContext.parallelize(['Hello', 'World', 'Python', 'Spark', 'Programming'])\n","\n","# Calculate lengths separately\n","lengths = Words.map(lambda x: len(x)).collect()\n","\n","# If you need to do more with these lengths, do it after collecting them\n","print(lengths)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":897},"id":"edN0eutokajz","executionInfo":{"status":"error","timestamp":1728113128616,"user_tz":-330,"elapsed":441,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"2a53cbd1-ca32-4d68-ab67-423e9fa65904"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/pyspark/serializers.py\", line 459, in dumps\n","    return cloudpickle.dumps(obj, pickle_protocol)\n","  File \"/usr/local/lib/python3.10/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n","    cp.dump(obj)\n","  File \"/usr/local/lib/python3.10/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n","    return Pickler.dump(self, obj)\n","  File \"/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\", line 386, in __getnewargs__\n","    raise PySparkRuntimeError(\n","pyspark.errors.exceptions.base.PySparkRuntimeError: [RDD_TRANSFORM_ONLY_VALID_ON_DRIVER] It appears that you are attempting to broadcast an RDD or reference an RDD from an \n","action or transformation. RDD transformations and actions can only be invoked by the \n","driver, not inside of other transformations; for example, \n","rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values \n","transformation and count action cannot be performed inside of the rdd1.map \n","transformation. For more information, see SPARK-5063.\n"]},{"output_type":"error","ename":"PicklingError","evalue":"Could not serialize object: PySparkRuntimeError: [RDD_TRANSFORM_ONLY_VALID_ON_DRIVER] It appears that you are attempting to broadcast an RDD or reference an RDD from an \naction or transformation. RDD transformations and actions can only be invoked by the \ndriver, not inside of other transformations; for example, \nrdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values \ntransformation and count action cannot be performed inside of the rdd1.map \ntransformation. For more information, see SPARK-5063.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     72\u001b[0m             )\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;31m# This method is called when attempting to pickle an RDD, which is always an error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         raise PySparkRuntimeError(\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0merror_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RDD_TRANSFORM_ONLY_VALID_ON_DRIVER\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPySparkRuntimeError\u001b[0m: [RDD_TRANSFORM_ONLY_VALID_ON_DRIVER] It appears that you are attempting to broadcast an RDD or reference an RDD from an \naction or transformation. RDD transformations and actions can only be invoked by the \ndriver, not inside of other transformations; for example, \nrdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values \ntransformation and count action cannot be performed inside of the rdd1.map \ntransformation. For more information, see SPARK-5063.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-81-a66b952059a3>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Calculate lengths separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# If you need to do more with these lengths, do it after collecting them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5468\u001b[0m             \u001b[0mprofiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5470\u001b[0;31m         wrapped_func = _wrap_function(\n\u001b[0m\u001b[1;32m   5471\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_jrdd_deserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5472\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   5266\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serializer should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5267\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5268\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5269\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5270\u001b[0m     return sc._jvm.SimplePythonFunction(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   5249\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5250\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5251\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5252\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5253\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBroadcastThreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Default 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: PySparkRuntimeError: [RDD_TRANSFORM_ONLY_VALID_ON_DRIVER] It appears that you are attempting to broadcast an RDD or reference an RDD from an \naction or transformation. RDD transformations and actions can only be invoked by the \ndriver, not inside of other transformations; for example, \nrdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values \ntransformation and count action cannot be performed inside of the rdd1.map \ntransformation. For more information, see SPARK-5063."]}]},{"cell_type":"code","source":["Words = spark1.sparkContext.parallelize(['Hello', 'World', 'Python', 'Spark', 'Programming'])\n","def custom_length(s):\n","    count = 0\n","    for _ in s:\n","        count += 1\n","    return count\n","\n","lengths = Words.map(custom_length).collect()\n","print(\"Lengths of words:\", lengths)\n","max_word = Words.reduce(lambda x, y: x if custom_length(x) >= custom_length(y) else y)\n","print(\"Word with the maximum length:\", max_word)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gVfDEp9qjYw-","executionInfo":{"status":"ok","timestamp":1728113283504,"user_tz":-330,"elapsed":866,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"513879c5-ce7a-4c7c-9d03-e81de5247367"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["Lengths of words: [5, 5, 6, 5, 11]\n","Word with the maximum length: Programming\n"]}]},{"cell_type":"markdown","source":["Q10)  Map the Registration numbers to corresponding branch. 58000 series BDA, 57000 series AIML,\n","38000 series VLSI, 39000 series ES, and 47000 series CDC. Given registration number, generate a\n","key-value pair of Registration Number and Corresponding Branch."],"metadata":{"id":"W-ZyBqa4lEfL"}},{"cell_type":"code","source":["register_number_RDD=spark.sparkContext.parallelize(['58001', '57032', '38017', '39012', '47005', '58020', '57008','10000'])\n","register_number_RDD.collect()\n","\n","def get_branch(register_number_RDD):\n","  if 58000<=int(register_number_RDD)<=58900:\n","    return 'BDA'\n","  elif 57000<=int(register_number_RDD)<=57900:\n","    return 'AIML'\n","  elif 38000<=int(register_number_RDD)<=38900:\n","    return 'VLSI'\n","  elif 39000<=int(register_number_RDD)<=39900:\n","    return 'ES'\n","  elif 47000<=int(register_number_RDD)<=47900:\n","    return 'CDC'\n","  else:\n","    return 'Unknown Branch'\n","\n","branch_RDD=register_number_RDD.map(lambda x:(x,get_branch(x)))\n","\n","\n","for reg, branch in branch_RDD.collect():\n","    print(f\"Registration Number: {reg} -> Branch: {branch}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KxPc9-DClEJN","executionInfo":{"status":"ok","timestamp":1728113366636,"user_tz":-330,"elapsed":476,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"b0058490-861c-4f37-b99e-96c8d2b03712"},"execution_count":87,"outputs":[{"output_type":"stream","name":"stdout","text":["Registration Number: 58001 -> Branch: BDA\n","Registration Number: 57032 -> Branch: AIML\n","Registration Number: 38017 -> Branch: VLSI\n","Registration Number: 39012 -> Branch: ES\n","Registration Number: 47005 -> Branch: CDC\n","Registration Number: 58020 -> Branch: BDA\n","Registration Number: 57008 -> Branch: AIML\n","Registration Number: 10000 -> Branch: Unknown Branch\n"]}]},{"cell_type":"markdown","source":["Q11) Text file contain numbers. Numbers are separated by one white space. There is no order to store\n","the numbers. One line may contain one or more numbers. Find the maximum, minimum, sum and\n","mean of numbers."],"metadata":{"id":"I40--_X_laVX"}},{"cell_type":"code","source":["txt = spark.sparkContext.textFile('/content/drive/MyDrive/BDA_1_Sem/ABD/ABD_Lab/Hadoop/RDD Lab assignments/num.txt')\n","txt.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OYDeDhyZlN_0","executionInfo":{"status":"ok","timestamp":1728116408735,"user_tz":-330,"elapsed":452,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"12ea8c28-8d1e-4e09-96c8-0947806f0c2b"},"execution_count":92,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['23.5 42 12 77 8.9 -3 0 45.6 19.1 22 99 5']"]},"metadata":{},"execution_count":92}]},{"cell_type":"code","source":["txt.flatMap(lambda x:x.split(' ')).map(lambda x:float(x)).max()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"isEeyf1wwqOw","executionInfo":{"status":"ok","timestamp":1728116486449,"user_tz":-330,"elapsed":749,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"19d6bd06-afd5-4218-8424-0b946322f618"},"execution_count":97,"outputs":[{"output_type":"execute_result","data":{"text/plain":["99.0"]},"metadata":{},"execution_count":97}]},{"cell_type":"code","source":["txt.flatMap(lambda x:x.split(' ')).map(lambda x:float(x)).min()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8SGlmpSqxNGA","executionInfo":{"status":"ok","timestamp":1728116485217,"user_tz":-330,"elapsed":459,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"bc032ba7-b018-422b-f3e7-3bab47cca23d"},"execution_count":96,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-3.0"]},"metadata":{},"execution_count":96}]},{"cell_type":"code","source":["txt.flatMap(lambda x:x.split(' ')).map(lambda x:float(x)).sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1i-pmJU5xPaA","executionInfo":{"status":"ok","timestamp":1728116496432,"user_tz":-330,"elapsed":944,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"6902d2df-f4f1-4f9f-b5c9-1e273ad0912f"},"execution_count":98,"outputs":[{"output_type":"execute_result","data":{"text/plain":["351.1"]},"metadata":{},"execution_count":98}]},{"cell_type":"code","source":["txt.flatMap(lambda x:x.split(' ')).map(lambda x:float(x)).mean()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h21PElJ9xR2A","executionInfo":{"status":"ok","timestamp":1728116502413,"user_tz":-330,"elapsed":459,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"73504428-d293-411c-a3f7-c1c5da13945b"},"execution_count":99,"outputs":[{"output_type":"execute_result","data":{"text/plain":["29.258333333333333"]},"metadata":{},"execution_count":99}]},{"cell_type":"markdown","source":["Q12) A text file (citizen.txt) contains data about citizens of country. Fields (information in file) are Name,\n","dob, Phone, email and state name. Another file contains mapping of state names to state code\n","like Karnataka is codes as KA, TamilNadu as TN, Kerala KL etc. Compress the citizen.txt file by\n","changing full state name to state code."],"metadata":{"id":"NTPTCTWNxUHy"}},{"cell_type":"code","source":["citi = spark.sparkContext.textFile('/content/drive/MyDrive/BDA_1_Sem/ABD/ABD_Lab/Hadoop/RDD Lab assignments/citizen.txt')\n","citi.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wVi8dATW1DmI","executionInfo":{"status":"ok","timestamp":1728118280298,"user_tz":-330,"elapsed":494,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"c40d268d-4ed1-4e6c-bfa6-55089554dc43"},"execution_count":103,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Ravi Kumar, 1990-07-15, 9876543210, ravi.kumar@example.com, Karnataka',\n"," 'Ananya Nair, 1985-06-22, 9123456780, ananya.nair@example.com, Kerala',\n"," 'Karthik Reddy, 1992-12-05, 8765432109, karthik.reddy@example.com, Andhra Pradesh',\n"," 'Priya Iyer, 1988-03-10, 9988776655, priya.iyer@example.com, Tamil Nadu',\n"," 'Nikhil Shetty, 1995-01-20, 9191919191, nikhil.shetty@example.com, Telangana',\n"," 'Deepa Mohan, 1993-09-25, 7654321098, deepa.mohan@example.com, Karnataka',\n"," 'Vikram Menon, 1980-04-15, 9456781234, vikram.menon@example.com, Kerala',\n"," 'Srinivas Rao, 1985-11-30, 9988776655, srinivas.rao@example.com, Andhra Pradesh',\n"," 'Lakshmi Ramesh, 1992-05-17, 9876543210, lakshmi.ramesh@example.com, Tamil Nadu',\n"," 'Arunachalam S, 1991-09-22, 9123456780, arunachalam.s@example.com, Tamil Nadu',\n"," 'Meera Krishnan, 1989-02-18, 8765432109, meera.krishnan@example.com, Kerala',\n"," 'Ganesh Babu, 1994-06-12, 7654321098, ganesh.babu@example.com, Karnataka',\n"," 'Shalini Pillai, 1993-04-30, 9191919191, shalini.pillai@example.com, Kerala',\n"," 'Vishnu Prasad, 1987-08-01, 9456781234, vishnu.prasad@example.com, Telangana',\n"," 'Geetha Menon, 1995-10-14, 8765432100, geetha.menon@example.com, Karnataka']"]},"metadata":{},"execution_count":103}]},{"cell_type":"code","source":["state = {\"Andhra Pradesh\": \"AP\", \"Telangana\": \"TS\",\"Karnataka\": \"KA\",\"TamilNadu\": \"TN\", \"Kerala\": \"KL\"}\n","state_dict_broadcast = spark.sparkContext.broadcast(state)\n","print(state_dict_broadcast.value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hjH-ycyM4FU-","executionInfo":{"status":"ok","timestamp":1728120438549,"user_tz":-330,"elapsed":465,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"c7ebc955-10c4-461a-d2a5-24a1370a211e"},"execution_count":115,"outputs":[{"output_type":"stream","name":"stdout","text":["{'Andhra Pradesh': 'AP', 'Telangana': 'TS', 'Karnataka': 'KA', 'TamilNadu': 'TN', 'Kerala': 'KL'}\n"]}]},{"cell_type":"code","source":["def replace_state(line):\n","    fields = line.split(',')  # Split by comma to get individual fields\n","    state_name = fields[-1].strip()  # State is the last field\n","    state_code = state_dict_broadcast.value.get(state_name, state_name)  # Get code or keep name if not found\n","    fields[-1] = state_code  # Replace the state name with the code\n","    return ', '.join(fields)  # Return the updated line\n","\n","# Transform the citizen data\n","compressed_citizen_rdd = citi.map(replace_state)\n","\n","# Print the transformed data\n","print(compressed_citizen_rdd.collect())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HqENResD_92P","executionInfo":{"status":"ok","timestamp":1728120512452,"user_tz":-330,"elapsed":456,"user":{"displayName":"Nikhil S G","userId":"11597562071480903754"}},"outputId":"97a70da9-0301-4c7e-ad94-43ddda6a0c8c"},"execution_count":116,"outputs":[{"output_type":"stream","name":"stdout","text":["['Ravi Kumar,  1990-07-15,  9876543210,  ravi.kumar@example.com, KA', 'Ananya Nair,  1985-06-22,  9123456780,  ananya.nair@example.com, KL', 'Karthik Reddy,  1992-12-05,  8765432109,  karthik.reddy@example.com, AP', 'Priya Iyer,  1988-03-10,  9988776655,  priya.iyer@example.com, Tamil Nadu', 'Nikhil Shetty,  1995-01-20,  9191919191,  nikhil.shetty@example.com, TS', 'Deepa Mohan,  1993-09-25,  7654321098,  deepa.mohan@example.com, KA', 'Vikram Menon,  1980-04-15,  9456781234,  vikram.menon@example.com, KL', 'Srinivas Rao,  1985-11-30,  9988776655,  srinivas.rao@example.com, AP', 'Lakshmi Ramesh,  1992-05-17,  9876543210,  lakshmi.ramesh@example.com, Tamil Nadu', 'Arunachalam S,  1991-09-22,  9123456780,  arunachalam.s@example.com, Tamil Nadu', 'Meera Krishnan,  1989-02-18,  8765432109,  meera.krishnan@example.com, KL', 'Ganesh Babu,  1994-06-12,  7654321098,  ganesh.babu@example.com, KA', 'Shalini Pillai,  1993-04-30,  9191919191,  shalini.pillai@example.com, KL', 'Vishnu Prasad,  1987-08-01,  9456781234,  vishnu.prasad@example.com, TS', 'Geetha Menon,  1995-10-14,  8765432100,  geetha.menon@example.com, KA']\n"]}]}]}