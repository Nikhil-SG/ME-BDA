{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["$${\\color{yellow}{\\text{Deep Learning for LLMs}}}$$\n","\n"],"metadata":{"id":"M7g7bxFCHxGP"}},{"cell_type":"markdown","source":["---\n","\n","Load essential libraries\n","\n","---"],"metadata":{"id":"0_BbyTKXQflD"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import torch\n","import matplotlib.pyplot as plt\n","plt.style.use('dark_background')\n","%matplotlib inline\n","import sys\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n","from sklearn.metrics import confusion_matrix\n","import gensim.downloader\n","import nltk\n","from nltk.tokenize import word_tokenize"],"metadata":{"id":"20W0d4ruQjE4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Mount Google Drive folder if running Google Colab\n","\n","---"],"metadata":{"id":"sfYXkqmLiVLM"}},{"cell_type":"code","source":["## Mount Google drive folder if running in Colab\n","if('google.colab' in sys.modules):\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount = True)\n","    DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE/Workshops/Generative AI with LLMs Workshop_December2024'\n","    DATA_DIR = DIR+'/Data/'\n","else:\n","    DATA_DIR = 'Data/'"],"metadata":{"id":"VYzBBBxqiaGa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","**We will now use Pytorch to create tensors**\n","\n","The patient data matrix:\n","\n","![patient data matrix](https://1drv.ms/i/s!AjTcbXuSD3I3hsxIkL4V93-CGq8RkQ?embed=1&width=660)\n","\n","**Notation**:\n","\n","Zeroth patient vector $\\mathbf{x}^{(0)}= \\begin{bmatrix}72\\\\120\\\\37.3\\\\104\\\\32.5\\end{bmatrix}$ and zeroth feature (heart rate vector) $\\mathbf{x}_0 = \\begin{bmatrix}72\\\\85\\\\68\\\\90\\\\84\\\\78\\end{bmatrix}.$\n","\n","---\n","\n"],"metadata":{"id":"avVZ6D1ZgEUT"}},{"cell_type":"code","source":["## Create a patient data matrix as a constant tensor\n","X = torch.tensor([[72, 120, 37.3, 104, 32.5],\n","                 [85, 130, 37.0, 110, 14],\n","                 [68, 110, 38.5, 125, 34],\n","                 [90, 140, 38.0, 130, 26],\n","                 [84, 132, 38.3, 146, 30],\n","                 [78, 128, 37.2, 102, 12]])\n","print(X)\n","print(X.shape)\n","# X is a rank-2 tensor which is similar to a numpy 2D array\n","print(X[0]) # this is patient-0 info which is a rank-1 tensor\n","print(X[0, 2])"],"metadata":{"id":"zrPnepAEvr0O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","**Convert a PyTorch object into a numpy array**\n","\n","---"],"metadata":{"id":"cevtn_b4gek5"}},{"cell_type":"code","source":["X_numpy = X.numpy()\n","print(X_numpy)\n","print(type(X_numpy))\n","print(X_numpy.shape)"],"metadata":{"id":"JrYQ2moygfPu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","**Addition and subtraction of vectors, scalar multiplication (apply operation componentwise)**\n","\n","![vector addition](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3NokBAAAAAZLAaAoWwhtn8Vk26NotALo?width=256)\n","\n","![vector subtracton](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3M4kBAAAAAU_n_mAEv006QFZm_sUj2Dc?width=256)\n","\n","![vector multiplication](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3NIkBAAAAAa_qL04bLT4kWoNeHcrR9LQ?width=256)\n","\n","![vector geometry1](https://1drv.ms/i/c/37720f927b6ddc34/IQSGNMr5z3SSRry7LSKL7LybAcGYuzgw5smabV8-6DudXIs?width=230)\n","\n","![vector geometry2](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3WokBAAAAAQi8FPV9YCebl5WnyEKJ3vg?width=213&height=192)\n","\n","\n","---"],"metadata":{"id":"QS3MmzwsgkWU"}},{"cell_type":"code","source":["# Vector addition\n","print(X[1, :] + X[2, :])\n","\n","# Vector subtraction\n","print(X[1, :] - X[2, :]) # how different patient-1 and patient-2 are\n","\n","# Scalar-vector multiplication\n","print(X[:, 2])\n","print((9/5)*X[:, 2] + 32)\n","\n","# Average patient\n","print((1/4)*(X[0, :] + X[1, :] + X[2, :] + X[3, :]))\n","print(torch.mean(X, dim = 0)) # dim = 0 means top-to-bottom operation or each row is an element"],"metadata":{"id":"TgPtJP0sglQP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Application of vector subtraction in natural language processing (NLP): download the word embedding model trained on Wikipedia articles.\n","\n","---"],"metadata":{"id":"1t_qXrlCROKA"}},{"cell_type":"code","source":["model = gensim.downloader.load('glove-wiki-gigaword-50')"],"metadata":{"id":"_e13FnW0RUwy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Now we will see what embedding vector comes as a result of applying the model for the words *cricket* and *football*.\n","\n","Next, we will do an *intuitive* subtraction of word embeddings as in\n","\n","1. Cricket without Tendulkar\n","2. Football without Messi\n","\n","Note that the embedding vectors have 50 components corresponding to the 50-dimensional embedding of model suggested by the name '**glove-wiki-gigaword-50**'\n","\n","---"],"metadata":{"id":"7YRVJferRlK5"}},{"cell_type":"code","source":["print(model['cricket'])\n","print(model['football'])\n","a = model['cricket'] - model['tendulkar']\n","b = model['football'] - model['messi']\n","print(a)\n","print(b)"],"metadata":{"id":"HVVFzeQyR3Wb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","A tensor of rank 3 corresponding to 4 time stamps (hourly), 3 samples (patients), 2 features (HR and BP)\n","\n","---"],"metadata":{"id":"8VPICS8ggvvg"}},{"cell_type":"code","source":["# A rank-3 patient tensor with shape (4, 3, 2)\n","# with meaning for\n","# axis-0 as 4 hourly timestamps,\n","# axis-1 as 3 patients, and\n","# axis-2 as 2 features (HR and BP)\n","T = torch.tensor([[[74., 128], [79, 116], [71, 116]],\n","                 [[78, 118], [82, 124], [72, 128]],\n","                 [[84, 138], [84, 130], [74, 120]],\n","                 [[82, 126], [76, 156], [82, 132]]])\n","print(T)\n","print(T.shape)"],"metadata":{"id":"qn6KT_pBgwUe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","**Accessing elements of a tensor**\n","\n","---"],"metadata":{"id":"JV0fpSojg2EZ"}},{"cell_type":"code","source":["## Accessing elements of a tensor\n","# Rank-3 tensor T has axes order (timestamps, patients, features)\n","\n","# Element of T at postion 3 w.r.t. axis-0, position 2 w.r.t. axis-1,\n","# position-1 w.r.t axis-2\n","print(T[3, 2, 1]) # 3rd timestamp, 2nd patient, 1st feature (BP)\n","\n","print(T[1]) # element-1 of object T which is also the info for all patients at 10AM (admission at 9AM)\n","\n","print(T[3, 2]) # patient-2 info at 12PM"],"metadata":{"id":"1GbZuDYqg22n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","$l_2$ norm or the geometric length of a vector denoted as $\\lVert \\mathbf{a}\\rVert$ tells us how long a vector is. In 2-dimensions, $$\\lVert \\mathbf{a}\\rVert_2 = \\sqrt{a_1^2+a_2^2}$$ and in $n$-dimensions, $$\\lVert \\mathbf{a}\\rVert_2 = \\sqrt{a_1^2+a_2^2+\\cdots+a_n^2}.$$\n","\n","![vector norm](https://1drv.ms/i/c/37720f927b6ddc34/IQT817WmpQjlRqZ1R0d5Cfv6AUW6c4robL-gk06i9wmCaFU?width=250)\n","\n","---"],"metadata":{"id":"gc9EJuZQhD9i"}},{"cell_type":"code","source":["## l2 norm of a vector\n","x = torch.tensor([76., 124])\n","print(x)\n","print(torch.norm(x)) # sqrt(76^2+124^2)"],"metadata":{"id":"OM65UP4_hEso"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","\n","**Dot Product of Vectors**\n","\n","A scalar resulting from an elementwise multiplication and addition: $$\\mathbf{a}{\\color{cyan}\\cdot}\\mathbf{b} = {\\color{red}{a_1b_1}}+{\\color{green}{a_2b_2}}+\\cdots+{\\color{magenta}{a_nb_n}}$$\n","\n","The <font color=\"cyan\">dot</font> ${\\color{cyan}\\cdot}$ represents the computation of the dot product.\n","\n","\n","---"],"metadata":{"id":"SRbanrUmwLX7"}},{"cell_type":"code","source":["## Dot product of vectors\n","a = torch.tensor([1., 2, 3])\n","b = torch.tensor([4., 5, 6])\n","print(torch.dot(a, b)) # elementwise product followed by a summation"],"metadata":{"id":"s91XY1JZwU2w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","The dot product is a measure of similarity between vectors (or, how aligned they are geometrically).\n","\n","![dot product](https://1drv.ms/i/c/37720f927b6ddc34/IQTbcGSjdbhSTJ7J39d5BCWAAWS6-y5U6J87vHuDWeAqGwM?width=450)\n","---"],"metadata":{"id":"2-b90m-QXyFp"}},{"cell_type":"code","source":["a = torch.tensor([1.0, 2.0])\n","b = torch.tensor([2.0, 4.0])  # b is exactly aligned with a\n","c = torch.tensor([-2.0, 1.0]) # c is perpendicular or orthogonal to a\n","d = torch.tensor([-1.0, -2.0])  # d is anti-aligned with a\n","print(torch.dot(a, b))\n","print(torch.dot(a, c))\n","print(torch.dot(a, d))"],"metadata":{"id":"3GxZ95uXXz3P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Cauchy-Schwarz inequality $-1\\leq\\frac{\\mathbf{x}\\cdot{\\mathbf{y}}}{\\lVert\\mathbf{x}\\rVert_2\\lVert\\mathbf{y}\\rVert_2}\\leq1.$\n","\n","This is a normalized measure of similarity (or extent of alignment) between vectors.\n","\n","Angle between vectors $\\mathbf{x}$ and $\\mathbf{y} = \\cos^{-1}\\left(\\frac{\\mathbf{x}\\cdot{\\mathbf{y}}}{\\lVert\\mathbf{x}\\rVert_2\\lVert\\mathbf{y}\\rVert_2}\\right).$\n","\n","![angle](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3WokBAAAAAQi8FPV9YCebl5WnyEKJ3vg?width=213&height=192)\n","\n","\n","---"],"metadata":{"id":"U6CS4_8byCs8"}},{"cell_type":"code","source":["x = torch.tensor([1.0, 2.0])\n","y = torch.tensor([2.0, 1.0])\n","print(torch.dot(x, y) / (torch.norm(x) * torch.norm(y))) # normalized similarity measure\n","print(torch.acos(torch.dot(x, y) / (torch.norm(x) * torch.norm(y)))) # angle in radians\n","print((180/torch.pi)*torch.acos(torch.dot(x, y) / (torch.norm(x) * torch.norm(y)))) # angle in degrees"],"metadata":{"id":"q4UhBnPUx7TV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Application of the Cauchy-Schwarz inequality: is \"Cricket without Tendulkar\" same as \"Football without Messi\"?\n","\n","---"],"metadata":{"id":"1bnmEkg3Tctx"}},{"cell_type":"code","source":["a = model['cricket'] - model['tendulkar']\n","b = model['football'] - model['messi']\n","print(np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))) # normalized similarity\n","print((180/np.pi)*np.arccos(np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b)))) # angular difference in degrees\n","print(np.linalg.norm(a-b)) # linear difference"],"metadata":{"id":"KrmCknO5TkNZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","\n","**Hadamard Product of Vectors**\n","\n","A vector resulting from an elementwise multiplication: $$\\mathbf{a}{\\color{cyan}\\otimes}\\mathbf{b} = \\begin{bmatrix}{\\color{red}{a_1\\times b_1}}\\\\{\\color{green}{a_2\\times b_2}}\\\\\\vdots\\\\{\\color{magenta}{a_n\\times b_n}}\\end{bmatrix}.$$\n","\n","The <font color=\"cyan\">$\\otimes$</font> represents the computation of the Hadamard product.\n","\n","---"],"metadata":{"id":"ayzM_0_synRF"}},{"cell_type":"code","source":["## Hadamard product\n","a = torch.tensor([1.0, 2.0, 3.0])\n","b = torch.tensor([4.0, 5.0, 6.0])\n","\n","# Element-wise multiplication (Hadamard product)\n","print(a * b)  # Using the * operator\n","print(torch.mul(a, b))  # Using torch.mul function"],"metadata":{"id":"UPojS0rIzR8p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","A matrix-vector product is simply a sequence of dot products of the rows of matrix (seen as vectors) with the vector\n","\n","![matrvec product](https://1drv.ms/i/c/37720f927b6ddc34/IQQ1cQ8fZdFmS4cnGkBlsZbAAaL2zMtzWdjHe-HCMt4UTA0?width=500)\n","\n","---"],"metadata":{"id":"oruyV_EjhqCR"}},{"cell_type":"code","source":["## Matrix-vector product\n","A = torch.tensor([[1.0, 2.0, 4.0],\n","                  [2.0, -1.0, 3.0]])\n","x = torch.tensor([4.0, 2.0, -2.0])\n","\n","# Matrix-vector multiplication\n","print(A)\n","print(x)\n","print(torch.matmul(A, x))"],"metadata":{"id":"A_IScSWzhpi7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Here we create a simple sentence in English and tokenize it\n","\n","---"],"metadata":{"id":"uTnGSJ3vT4EN"}},{"cell_type":"code","source":["sentence = 'i swam quickly across the river to get to the other bank'\n","nltk.download('punkt_tab')\n","tokens = word_tokenize(sentence)\n","print(len(tokens))\n","print(tokens)"],"metadata":{"id":"pQ73kkevT5L3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Generate the word embeddings for the tokens and store them in a matrix $\\mathbf{X}$ such that each row of the matrix corresponds to a token.\n","\n","---"],"metadata":{"id":"M40pqI8UUbX4"}},{"cell_type":"code","source":["X_word = torch.tensor(model[tokens])\n","np.set_printoptions(precision=3, suppress=True)\n","print(X_word)\n","print(X_word.shape)\n","print(X_word[1]) # embedding vector for the word \"swam\""],"metadata":{"id":"1mKKVRyxUh5V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","A matrix-matrix product is simply a sequence of matrix-vector products.\n","\n","![matmatprod](https://1drv.ms/i/c/37720f927b6ddc34/IQQ-B3z7tbWHQqBrW9k2ElDVAUc5fWzM24txLkgBK7f8Yac?width=350)\n","\n","\n","---"],"metadata":{"id":"0Z0pZQisxtY-"}},{"cell_type":"code","source":["## Matrix-matrix product\n","A = torch.tensor([[1.0, 2.0, 4.0],\n","                  [2.0, -1.0, 3.0]])\n","B = torch.tensor([[4.0, -1.0],\n","                  [2.0, 0.0],\n","                  [-2.0, 3.0]])\n","torch.matmul(A, B)"],"metadata":{"id":"YSg1brJ9yKnM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","The similarity between each pair of words represented in the word embeddings matrix $\\mathbf{X}$ is the matrix-matrix product $\\mathbf{X}\\mathbf{X}^\\mathrm{T}.$\n","\n","---"],"metadata":{"id":"mVoJRc6kUtI2"}},{"cell_type":"code","source":["S = torch.matmul(X_word, X_word.T)\n","print(S)\n","print(S.shape)"],"metadata":{"id":"ms9Qg5AoVJy_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Matrix-matrix product using the patient data matrix and a weights matrix:\n","\n","![Patient dataset](https://1drv.ms/i/s!AjTcbXuSD3I3hscharGu916tjWNzZQ?embed=1&width=660)\n","\n","---"],"metadata":{"id":"QH_sW6XW0MDT"}},{"cell_type":"code","source":["# Patients data matrix\n","X = torch.tensor([[72, 120, 36.5],\n","                  [85, 130, 37.0],\n","                  [68, 110, 38.5],\n","                  [90, 140, 38.0]])\n","print(X)\n","\n","# Weights matrix\n","W = torch.tensor([[0.5, 0.3, -0.6],\n","                  [0.9, 0.3, -0.25],\n","                  [-1.5, 0.4, 0.1]])\n","print(W)\n","\n","# Raw scores matrix (Matrix-matrix multiplication)\n","Z = torch.matmul(X, W) # PyTorch matmul() also does matrix-matrix multiplication\n","print(Z)\n","\n","# The raw scores are also referred to as the logits"],"metadata":{"id":"s0RFtdhxhkvZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","The softmax function\n","\n","![softmax](https://1drv.ms/i/s!AjTcbXuSD3I3hscmdol7J2G4GDo5WQ?embed=1&width=660)\n","\n","---"],"metadata":{"id":"sOnZvS5Vjjrd"}},{"cell_type":"code","source":["## In-built softmax function in PyTorch (dim = 1 corresponds to applying row-by-row)\n","## applied to the word embeddings similarity matrix\n","S_softmax = torch.nn.functional.softmax(torch.tensor(S), dim = 1)\n","print(S_softmax[1])"],"metadata":{"id":"TDkPV0qXVVyp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Transform the word embeddings using the softmax-normalized similarity matrix.\n","\n","---"],"metadata":{"id":"_OVqwridV5T_"}},{"cell_type":"code","source":["X_word = torch.tensor(model[tokens])\n","Y = torch.matmul(S_softmax, X_word)\n","print(Y)"],"metadata":{"id":"SF731yR3V3E7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## In-built softmax function in PyTorch (dim = 1 corresponds to row-by-row)\n","## applied to the toy patient data matrix\n","softmax_scores = torch.nn.functional.softmax(Z, dim = 1)\n","print(softmax_scores)"],"metadata":{"id":"0EG_bqPgjlV5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","A toy data matrix with output labels and an initial weights matrix for the softmax classifier:\n","\n","![data for softmax](https://1drv.ms/i/s!AjTcbXuSD3I3hspfrgklysOtJMOjaA?embed=1&width=660)\n","\n","---"],"metadata":{"id":"6_8cPXFFkiUZ"}},{"cell_type":"code","source":["# Create the data matrix (read from a file typically)\n","X = np.array([[72, 120, 37.3, 104, 32.5],\n","              [85, 130, 37.0, 110, 14],\n","              [68, 110, 38.5, 125, 34],\n","              [90, 140, 38.0, 130, 26],\n","              [84, 132, 38.3, 146, 30],\n","              [78, 128, 37.2, 102, 12]])\n","\n","# Standardize the data matrix\n","sc = StandardScaler()\n","X_S = sc.fit_transform(X)  # fit(), fit_transform(), transform()\n","\n","# Convert to a PyTorch tensor\n","X_S = torch.tensor(X_S, dtype=torch.float32)\n","\n","# Get the number of samples and features\n","num_samples, num_features = X_S.shape\n","\n","# Create the output labels vector (also read from a file typically)\n","y = np.array(['non-diabetic',\n","              'diabetic',\n","              'non-diabetic',\n","              'pre-diabetic',\n","              'diabetic',\n","              'pre-diabetic'])\n","\n","# One-hot encoding of output labels using scikit-learn\n","ohe = OneHotEncoder(sparse_output=False)  # Use `sparse_output=False` for dense array\n","Y = ohe.fit_transform(y.reshape(-1, 1))\n","\n","# Convert to a PyTorch tensor\n","Y = torch.tensor(Y, dtype=torch.float32)\n","\n","# Get the number of labels\n","num_labels = Y.shape[1]\n","\n","# Create the weights matrix\n","W = torch.tensor([[-0.1, 0.5, 0.3],\n","                  [0.9, 0.3, 0.5],\n","                  [-1.5, 0.4, 0.1],\n","                  [0.1, 0.1, -1.0],\n","                  [-1.2, 0.5, -0.8]], dtype=torch.float32)\n","\n","print(X_S)\n","print(Y)\n","print(W)"],"metadata":{"id":"MJ3U-JCukmIG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Bias trick to absorb the bias into the weights matrix\n","\n","![bias trick](https://1drv.ms/i/c/37720f927b6ddc34/IQR8NDbhvaddQa3W3F_46q4nATD7WBNgnwGJ7QC6HDL6g14?width=550)\n","\n","---"],"metadata":{"id":"1cQDyu7llDo9"}},{"cell_type":"code","source":["## Bias trick to absorb the bias into the weights matrix\n","# Concatenate a column of ones to X_S (bias term)\n","X_B = torch.cat([X_S, torch.ones((num_samples, 1))], dim=1)\n","\n","# Create the bias vector `b`\n","b = 0.1 * torch.ones((1, num_labels))\n","\n","# Concatenate the weights matrix `W` with the bias vector `b`\n","W_B = torch.cat([W, b], dim=0)\n","\n","print(X_B)\n","print(W_B)"],"metadata":{"id":"DlviiS0tlH7p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Forward propagation for the toy patient dataset: $$\\textbf{bias-added input }\\mathbf{X}_B\\,{\\color{yellow}\\longrightarrow}\\,\\textbf{raw scores }\\mathbf{Z}=\\mathbf{X}_B\\textbf{W}_B\\,{\\color{yellow}\\longrightarrow}\\,\\textbf{softmax activated scores }\\mathbf{A}=\\text{softmax}(\\mathbf{Z}).$$\n","\n","---"],"metadata":{"id":"5rZkNr8d1gAw"}},{"cell_type":"code","source":["# Raw scores matrix\n","Z = torch.matmul(X_B, W_B) # also alled logits\n","print(Z)\n","\n","# Softmax activated scores\n","A = torch.nn.functional.softmax(Z, dim = 1)\n","\n","# Predicted probabilities for each sample\n","print(A)\n","\n","# True output label for each sample\n","print(Y)"],"metadata":{"id":"A5g-D7NLlZBl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Loss for each sample can be quantified using the categorical crossentropy (CCE) loss function which is defined as $$\\color{yellow}{-\\log(\\text{predicted probability that a sample belongs its correct class})}$$\n","\n","For example, consider a sample with\n","\n","- true_label = [$\\color{yellow}{1}$ 0 0]\n","- predicted_label = [$\\color{yellow}{0.05}$, 0.99, 0.05]\n","\n","categorical crossentropy loss = $-\\log(\\color{yellow}{0.05}).$\n","\n","Here, we calculate the average CCE loss for all all samples and average them out.\n","\n","---"],"metadata":{"id":"dgtOD11HljXv"}},{"cell_type":"code","source":["## Calculate average CCE loss\n","loss = torch.mean(-torch.log(torch.sum(Y * A, dim = 1)))\n","print(loss)\n","\n","# Using the PyTorch in-built function for CCE loss\n","loss_fn = torch.nn.CrossEntropyLoss()\n","loss = loss_fn(Z, torch.argmax(Y, dim = 1))\n","print(loss)"],"metadata":{"id":"aiCY-GvwlpDt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Applying the gradient descent method with\n","\n","- a maximum number of iterations equal to 1000\n","- a stopping tolerance equal to $10^{-6}$\n","- a learning rate of 0.01\n","\n"," to minimize $$L(\\mathbf{w}) = (w_1-2)^2+(w_2+3)^2$$ starting from $\\mathbf{w} = \\begin{bmatrix}w_1\\\\w_2\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\end{bmatrix}.$\n","\n","---"],"metadata":{"id":"Iro8wo7tnv2g"}},{"cell_type":"code","source":["# Initialize weights as tensors with gradients\n","w = torch.tensor([0.0, 0.0], requires_grad=True)\n","\n","# Hyperparameters\n","maxiter = 1000\n","tol = 1e-06\n","lr = 1e-02\n","norm_grad = float('inf')\n","\n","k = 0\n","while k < maxiter and norm_grad > tol:\n","    # Zero the gradients\n","    if w.grad is not None:\n","        w.grad.zero_()\n","\n","    # Define the loss function\n","    L = (w[0] - 2)**2 + (w[1] + 3)**2\n","\n","    # Backpropagate to compute gradients\n","    L.backward()\n","\n","    # Update weights using gradient descent\n","    with torch.no_grad():\n","        w -= lr * w.grad\n","\n","    # Compute the norm of the gradient\n","    norm_grad = w.grad.norm().item()\n","    k += 1\n","\n","    print(f'Iteration {k}: ||grad|| = {norm_grad}')"],"metadata":{"id":"qcM3YnJmnwrY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","We will consider again the same toy data matrix with 6 samples and 3 possible output labels :\n","\n","\n","![data for softmax](https://1drv.ms/i/s!AjTcbXuSD3I3hsxIkL4V93-CGq8RkQ?embed=1&width=660)\n","\n","---"],"metadata":{"id":"IvRw58l8p2T3"}},{"cell_type":"markdown","source":["---\n","\n","Define the linear layer (dense layer) where the raw scores are calculated through the linear operation:\n","$$\\underbrace{\\mathbf{Z}}_{\\color{red}{6\\times3}} = \\underbrace{\\begin{bmatrix}{\\mathbf{z}^{(0)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(1)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(2)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(3)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(4)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(5)}}^\\mathrm{T}\\end{bmatrix}}_{\\color{red}{6\\times3}}=\\underbrace{\\begin{bmatrix}{\\mathbf{x}^{(0)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(1)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(2)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(3)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(4)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(5)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\end{bmatrix}}_{\\color{red}{6\\times3}} = \\underbrace{\\begin{bmatrix}{\\mathbf{x}^{(0)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(1)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(2)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(3)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(4)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(5)}}^\\mathrm{T}\\mathbf{W}\\end{bmatrix}}_{\\color{red}{6\\times3}} + \\underbrace{\\begin{bmatrix}{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\end{bmatrix}}_{\\color{red}{6\\times3}}=\\underbrace{\\begin{bmatrix}{\\mathbf{x}^{(0)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(1)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(2)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(3)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(4)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(5)}}^\\mathrm{T}\\end{bmatrix}}_{\\color{red}{6\\times5}}\\underbrace{\\mathbf{W}}_{\\color{red}{5\\times3}}=\\underbrace{\\underbrace{\\mathbf{X}}_{6\\times 5}\\underbrace{\\mathbf{W}}_{5\\times 3}}_{\\color{red}{6\\times3}} + \\underbrace{\\begin{bmatrix}{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\end{bmatrix}}_{\\color{red}{6\\times3}}.$$\n","\n","---"],"metadata":{"id":"1ylibPssqTWs"}},{"cell_type":"code","source":["class LinearLayer(torch.nn.Module):\n","    def __init__(self, input_dim, nodes = 2):\n","        super(LinearLayer, self).__init__()  # Initialize the parent class (nn.Module)\n","        self.nodes = nodes\n","        # Define the weights and bias as parameters\n","        self.W = torch.nn.Parameter(torch.randn(input_dim, self.nodes))\n","        torch.nn.init.xavier_uniform_(self.W)  # Xavier uniform initialization\n","        self.b = torch.nn.Parameter(torch.randn(self.nodes))  # Random Normal initialization\n","\n","    def forward(self, input):\n","        # Linear transformation (input * W + b)\n","        output = torch.matmul(input, self.W) + self.b\n","        return output"],"metadata":{"id":"vt14T0bgqUxF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Defining a LinearLayer object and calling the forward() method applied to the toy patient data matrix.\n","\n","---"],"metadata":{"id":"4Sb0TWaWqosF"}},{"cell_type":"code","source":["layer1 = LinearLayer(num_features, 3)\n","print(layer1.W)\n","print(layer1.b)\n","layer1.forward(torch.tensor(X_S, dtype = torch.float32))"],"metadata":{"id":"XNrvxsYgqpve"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Define the softmax layer\n","\n","---"],"metadata":{"id":"4P8i-XsrroCB"}},{"cell_type":"code","source":["class SoftmaxLayer(torch.nn.Module):\n","    def __init__(self):\n","        super(SoftmaxLayer, self).__init__()\n","        self.activation = torch.nn.Softmax(dim = 1)\n","\n","    def forward(self, input):\n","        output = self.activation(input)  # Apply softmax activation\n","        return output"],"metadata":{"id":"DoACMi5FrokZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Defining a SoftmaxLayer object and calling the forward() method applied to the toy patient data matrix.\n","\n","---"],"metadata":{"id":"wiSxk97fr4xJ"}},{"cell_type":"code","source":["actlayer1 = SoftmaxLayer()\n","print(actlayer1.activation)\n","actlayer1.forward(layer1.forward(torch.tensor(X_S, dtype = torch.float32)))"],"metadata":{"id":"pn_dXu7xr7HV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Define the softmax classifier model\n","\n","---"],"metadata":{"id":"JRQ0SwBxs3WL"}},{"cell_type":"code","source":["class SoftmaxClassifierModel(torch.nn.Module):\n","    def __init__(self, input_dim, nodes=2):\n","        super(SoftmaxClassifierModel, self).__init__()\n","        self.nodes = nodes\n","        self.linearLayer = LinearLayer(input_dim, self.nodes)  # Linear layer\n","        self.softmaxLayer = SoftmaxLayer()  # Softmax activation layer\n","\n","    def forward(self, input):\n","        output = self.linearLayer(input)  # Forward pass through the linear layer\n","        output = self.softmaxLayer(output)  # Apply softmax activation\n","        return output"],"metadata":{"id":"tKicexaas31_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Perform forward propagation to the toy patient dataset using the SoftmaxClassifierModel built above.\n","\n","---"],"metadata":{"id":"upcq6QXttB8y"}},{"cell_type":"code","source":["model = SoftmaxClassifierModel(num_features, 3)\n","print(model(torch.tensor(X_S, dtype = torch.float32)))"],"metadata":{"id":"JQIET4VFtNSA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Define loss function (categorical crossentropy).\n","\n","---"],"metadata":{"id":"V3G_qHiVtWvj"}},{"cell_type":"code","source":["def loss_fn(true_labels, predicted_probs):\n","  loss = torch.mean(-torch.log(torch.sum(true_labels * predicted_probs, dim = 1)))\n","  return(loss)"],"metadata":{"id":"t-QsTVjRtYAh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Apply the softmax classifier model to the toy data set and calculate the loss.\n","\n","---"],"metadata":{"id":"_LpnJT06u5KV"}},{"cell_type":"code","source":["## Apply the softmax classifier model to the toy data set and calculate the loss\n","# Instantiate the model object\n","model = SoftmaxClassifierModel(num_features, 3) # invokes the constructor and sets up the layers\n","\n","# Calculate average data loss\n","loss_fn(Y, model(torch.tensor(X_S, dtype = torch.float32)))"],"metadata":{"id":"ih-XPwbdu5my"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Softmax classifier for the [MNIST](https://www.tensorflow.org/datasets/catalog/mnist) dataset\n","\n","---"],"metadata":{"id":"s4A6iMHOzAAO"}},{"cell_type":"code","source":["## Load MNIST data (note that shape of X_train and y_train)\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n","print(X_train.shape)\n","print(y_train.shape)"],"metadata":{"id":"Q6Wc0cWFzI7X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Reshape X_train and X_test such that the samples are along the rows\n","X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n","X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])"],"metadata":{"id":"cP47GmErzKzV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Problem parameters\n","num_samples_train = X_train_reshaped.shape[0]\n","num_samples_test = X_test_reshaped.shape[0]\n","num_features = X_train_reshaped.shape[1]\n","num_labels = len(np.unique(y_train))\n","print(f'No. of training samples = {num_samples_train},\\\n"," No. of test samples = {num_samples_test}, \\\n"," no. of features = {num_features}, no. of labels = {num_labels}')"],"metadata":{"id":"z54AQD5rzO12"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## One-hot encode output labels using scikit-learn (observe the shape of Y_train)\n","ohe = OneHotEncoder(sparse_output=False)\n","Y_train = torch.tensor(ohe.fit_transform(y_train.reshape(-1, 1)), dtype = torch.float32)\n","Y_test = torch.tensor(ohe.transform(y_test.reshape(-1, 1)), dtype = torch.float32)"],"metadata":{"id":"yeTfB5-xzSGi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Min-max scale the images using scikit-learn\n","mms = MinMaxScaler()\n","X_train_reshaped_scaled = torch.tensor(mms.fit_transform(X_train_reshaped), dtype=torch.float32)\n","X_test_reshaped_scaled = torch.tensor(mms.transform(X_test_reshaped), dtype=torch.float32)"],"metadata":{"id":"FCD_yzX1zUE0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Train the softmax classifier on the MNIST dataset\n","\n","---"],"metadata":{"id":"2OpSKsccees6"}},{"cell_type":"code","source":["## Train the softmax classifier on the MNIST dataset\n","# Initialize model\n","model = SoftmaxClassifierModel(num_features, num_labels)\n","\n","# Gradient descent\n","maxiter = 250\n","lr = 1e-03\n","\n","# Define loss function (CrossEntropyLoss in PyTorch includes softmax)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","# Define optimizer (RMSprop)\n","optimizer = torch.optim.RMSprop(model.parameters(), lr = lr)\n","\n","# Lists to store training and test losses\n","loss_train = [None] * maxiter\n","loss_test = [None] * maxiter\n","\n","# Start training loop\n","for k in range(maxiter):\n","    model.train()  # Set model to training mode\n","\n","    # Forward pass: compute predicted probabilities\n","    Yhat = model(X_train_reshaped_scaled)  # predicted probabilities\n","\n","    # Compute training loss\n","    L_train = loss_fn(Y_train, Yhat)  # CrossEntropyLoss expects raw logits (no softmax needed)\n","\n","    # Append training and test loss values\n","    loss_train[k] = L_train.item()  # Convert to scalar\n","    model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():  # Disable gradient calculation for testing\n","        Yhat_test = model(X_test_reshaped_scaled)  # predicted probabilities for test set\n","        L_test = loss_fn(Y_test, Yhat_test)  # Compute test loss\n","    loss_test[k] = L_test.item()  # Convert to scalar\n","\n","    # Print losses\n","    print(f'Iteration {k+1}, Training loss = {loss_train[k]}, Test loss = {loss_test[k]}')\n","\n","    # Backward pass: compute gradients\n","    optimizer.zero_grad()  # Zero the gradients before the backward pass\n","    L_train.backward()  # Backpropagate the gradients\n","\n","    # Update model parameters using optimizer\n","    optimizer.step()  # Perform one optimization step"],"metadata":{"id":"00eKgH6ez7Ix"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Plot training and test loss in the same figure\n","\n","---"],"metadata":{"id":"Yd8PGQKs15kB"}},{"cell_type":"code","source":["## Plot the training and test loss\n","fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n","ax.plot(loss_train, 'b', label = 'Train')\n","ax.plot(loss_test, 'r', label = 'Test')\n","ax.set_xlabel('Iteration')\n","ax.set_ylabel('Loss')\n","ax.legend();"],"metadata":{"id":"HL1069mE16U_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Assess model performance on test data\n","\n","---"],"metadata":{"id":"Eqxn_hXR2BM9"}},{"cell_type":"code","source":["## Assess model performance on test data\n","Yhat = model(X_test_reshaped_scaled)\n","\n","ypred = np.array(torch.argmax(Yhat, axis = 1)) # predicted labels for the test samples\n","ytrue = np.array(torch.argmax(Y_test, axis = 1)) # true labels for the test samples\n","print('Accuracy on test data = %3.2f'%(np.mean(ytrue == ypred)*100))\n","# Print confusion matrix\n","print(confusion_matrix(ytrue, ypred))"],"metadata":{"id":"AVdiXg6q2B4I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Plot a random test sample with its predicted label printed above the plot\n","test_index = np.random.choice(X_test.shape[0])\n","fig, ax = plt.subplots(1, 1, figsize = (2, 2))\n","print(f'Image classified as {ypred[test_index]}')\n","ax.imshow(tf.reshape(X_test_reshaped_scaled[test_index], [28, 28]).numpy(), cmap = 'gray');"],"metadata":{"id":"11ubRM9p2W3A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Define a nonlinear activation layer with ReLU activation\n","\n","---"],"metadata":{"id":"brjKnRYGbz6x"}},{"cell_type":"code","source":["class ReLULayer(torch.nn.Module):\n","    def __init__(self):\n","        super(ReLULayer, self).__init__()\n","        self.activation = torch.nn.ReLU()\n","\n","    def forward(self, input):\n","        output = self.activation(input)  # Apply softmax activation\n","        return output"],"metadata":{"id":"qKu3VuDGb41Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Define a one hidden layer neural network model\n","\n","---"],"metadata":{"id":"NrbyHyBXcRGj"}},{"cell_type":"code","source":["class NeuralNetworkModel(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_nodes = 2, nodes = 2):\n","        super(NeuralNetworkModel, self).__init__()\n","        self.hidden_nodes = hidden_nodes\n","        self.nodes = nodes\n","        self.linearLayer1 = LinearLayer(input_dim, self.hidden_nodes)  # 1st Linear layer\n","        self.actlayer1 = ReLULayer() # 1st activation layer (ReLU)\n","        self.linearLayer2 = LinearLayer(self.hidden_nodes, self.nodes)  # 2nd Linear layer\n","        self.softmaxLayer = SoftmaxLayer()  # Softmax activation layer\n","\n","    def forward(self, input):\n","        output = self.linearLayer1(input)  # Forward pass through the 1st linear layer\n","        output = self.actlayer1(output) # ReLU activation\n","        output = self.linearLayer2(output)  # Forward pass through the 2nd linear layer\n","        output = self.softmaxLayer(output)  # Apply softmax activation\n","        return output"],"metadata":{"id":"3ByD0iWjcXQF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Perform forward propagation to the toy patient dataset using the NeuralNetworkModel built above.\n","\n","---"],"metadata":{"id":"r3XJ5qcjdPaE"}},{"cell_type":"code","source":["model = NeuralNetworkModel(num_features, 4, 3) # 4 nodes in hidden layer\n","print(model(torch.tensor(X_S, dtype = torch.float32)))"],"metadata":{"id":"skQYdCkadfCF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Train the 1-hidden layer neural network classifier on the MNIST dataset\n","\n","---"],"metadata":{"id":"Ie2z0taWekVQ"}},{"cell_type":"code","source":["# This is an exercise## Train the softmax classifier on the MNIST dataset\n","# Initialize model\n","model = NeuralNetworkModel(num_features, 4, num_labels)\n","\n","# Gradient descent\n","maxiter = 1000\n","lr = 1e-03\n","\n","# Define loss function (CrossEntropyLoss in PyTorch includes softmax)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","# Define optimizer (RMSprop)\n","optimizer = torch.optim.RMSprop(model.parameters(), lr = lr)\n","\n","# Lists to store training and test losses\n","loss_train = [None] * maxiter\n","loss_test = [None] * maxiter\n","\n","# Start training loop\n","for k in range(maxiter):\n","    model.train()  # Set model to training mode\n","\n","    # Forward pass: compute predicted probabilities\n","    Yhat = model(X_train_reshaped_scaled)  # predicted probabilities\n","\n","    # Compute training loss\n","    L_train = loss_fn(Y_train, Yhat)  # CrossEntropyLoss expects raw logits (no softmax needed)\n","\n","    # Append training and test loss values\n","    loss_train[k] = L_train.item()  # Convert to scalar\n","    model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():  # Disable gradient calculation for testing\n","        Yhat_test = model(X_test_reshaped_scaled)  # predicted probabilities for test set\n","        L_test = loss_fn(Y_test, Yhat_test)  # Compute test loss\n","    loss_test[k] = L_test.item()  # Convert to scalar\n","\n","    # Print losses\n","    print(f'Iteration {k+1}, Training loss = {loss_train[k]}, Test loss = {loss_test[k]}')\n","\n","    # Backward pass: compute gradients\n","    optimizer.zero_grad()  # Zero the gradients before the backward pass\n","    L_train.backward()  # Backpropagate the gradients\n","\n","    # Update model parameters using optimizer\n","    optimizer.step()  # Perform one optimization step"],"metadata":{"id":"EWbCrpERen2d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","The following sequence of code snippets are mostly derived from Andrej Karpathy's superb tutorial on *Let's build GPT: from scratch, in code, spelled out* available via https://youtu.be/kCc8FmEb1nY?feature=shared\n","\n","Additional modifications are done to elaborate on the details from Karpathy's tutorial\n","\n","---"],"metadata":{"id":"de1ASiTGTDOe"}},{"cell_type":"markdown","source":["---\n","\n","Load, print, and look at the Shakespeare dataset\n","\n","---"],"metadata":{"id":"RfkgQKIMTYMe"}},{"cell_type":"code","source":["# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n","!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"],"metadata":{"id":"EakRJEwsTd2u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read it in to inspect it\n","with open('input.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()"],"metadata":{"id":"d1oPbDa6Tq9R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"length of dataset in characters: \", len(text))"],"metadata":{"id":"rnC0K17iTxIh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's look at the first 1000 characters\n","print(text[:1000])"],"metadata":{"id":"mDv8dANRTyyy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","print(''.join(chars))\n","print(vocab_size)"],"metadata":{"id":"rOTE1Ur1T2TP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","print(encode(\"hii there\"))\n","print(decode(encode(\"hii there\")))"],"metadata":{"id":"GCwf-Ry9T7VY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","ChatGPT's tiktoken library\n","\n","---"],"metadata":{"id":"SFPIE9yVT_2E"}},{"cell_type":"code","source":["# ChatGPTs tiktoken library (codebook size 50257)\n","!pip install tiktoken\n","import tiktoken"],"metadata":{"id":"uwDpzfgwUE2G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Using ChatGPTs tiktoken to tokenize\n","enc =tiktoken.get_encoding('gpt2')\n","print(enc.n_vocab)\n","enc.encode(\"hii there\")"],"metadata":{"id":"4L4nf5dWUHmT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's now encode the entire Shakespeare dataset and store it into a torch.Tensor\n","data = torch.tensor(encode(text), dtype = torch.long)\n","print(data.shape, data.dtype)\n","print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"],"metadata":{"id":"4xntJrdiULJR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's now split up the data into train and validation sets\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]"],"metadata":{"id":"Pm5HdXYDUXQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["block_size = 8\n","train_data[:block_size+1]"],"metadata":{"id":"wGZ6cW60Uaif"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = train_data[:block_size]\n","y = train_data[1:block_size+1]\n","for t in range(block_size):\n","    context = x[:t+1]\n","    target = y[t]\n","    print(f\"when input is {context} the target: {target}\")"],"metadata":{"id":"bo9P6UiwUdD5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(1337)\n","batch_size = 4 # how many independent sequences will we process in parallel?\n","block_size = 8 # what is the maximum context length for predictions?\n","\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    return x, y\n","\n","xb, yb = get_batch('train')\n","print('inputs:')\n","print(xb.shape)\n","print(xb)\n","print('targets:')\n","print(yb.shape)\n","print(yb)\n","\n","print('----')\n","\n","for b in range(batch_size): # batch dimension\n","    for t in range(block_size): # time dimension\n","        context = xb[b, :t+1]\n","        target = yb[b,t]\n","        print(f\"when input is {context.tolist()} the target: {target}\")"],"metadata":{"id":"HMMIFNDpUfSN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","A quick introduction to embeddings in PyTorch: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n","\n","---"],"metadata":{"id":"uzM9Bwp1Uiq4"}},{"cell_type":"code","source":[],"metadata":{"id":"4G2kMugXUqlx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Build a simple Bigram model class\n","\n","---"],"metadata":{"id":"DIZU0rJVUsC6"}},{"cell_type":"code","source":["torch.manual_seed(1337)\n","\n","class BigramLanguageModel(torch.nn.Module):\n","\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = torch.nn.Embedding(vocab_size, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        logits = self.token_embedding_table(idx) # (B,T,C)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = torch.nn.functional.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # get the predictions\n","            logits, loss = self(idx)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = torch.nn.functional.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n","\n","m = BigramLanguageModel(vocab_size)\n","logits, loss = m(xb, yb)\n","print(logits.shape)\n","print(loss)\n","\n","print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"],"metadata":{"id":"mfpGkNztU0TL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","Train the bigram model\n","\n","---"],"metadata":{"id":"zy984wO9VCNi"}},{"cell_type":"code","source":["# Optimizer\n","optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n","\n","batch_size = 32\n","for steps in range(10000): # increase number of steps for good results...\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = m(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","print(loss.item())"],"metadata":{"id":"BVh6HIDmVELI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Time to generate next set of tokens using the trained Bigram model\n","\n","---"],"metadata":{"id":"NMDLEYORVZL7"}},{"cell_type":"code","source":["print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"],"metadata":{"id":"_NgmH2oLVVeA"},"execution_count":null,"outputs":[]}]}