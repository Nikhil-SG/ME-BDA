{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install marker-pdf\n",
    "!pip install fpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U  llama-index==0.11.3 llama-index-llms-groq==0.2.0 llama-index-readers-smart-pdf-loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U llama-index-vector-stores-chroma llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.output import text_from_rendered\n",
    "from fpdf import FPDF  # Use fpdf2 (install with `pip install fpdf2`)\n",
    "\n",
    "# Directories for input PDFs and output cleaned PDFs\n",
    "input_directory = '/content/'  # Input folder where your PDF files are located\n",
    "output_directory = '/content/output_pdfs/'  # Output folder for cleaned PDFs\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Initialize the PDF converter from the marker library\n",
    "converter = PdfConverter(artifact_dict=create_model_dict())\n",
    "\n",
    "# Function to clean the extracted text using regex\n",
    "def clean_text(text):\n",
    "    # Remove unwanted characters like pipe '|' and triple newlines\n",
    "    text = re.sub(r'\\|', '', text)\n",
    "    text = re.sub(r'\\n\\n\\n', '\\n', text)\n",
    "    text = re.sub(r'\\n\\n', '\\n', text)\n",
    "    text = re.sub(r'\\u2013|\\u2014', '-', text)  # Replace en-dash and em-dash with hyphen\n",
    "    text = re.sub(r'\\u2022|\\uf0b7', '-', text)  # Replace bullet points with hyphen\n",
    "    return text\n",
    "\n",
    "# Function to save the cleaned text as a new PDF using `fpdf2`\n",
    "def save_text_as_pdf(text, output_pdf_path):\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "    # Write the text to the PDF\n",
    "    pdf.multi_cell(0, 10, txt=text)  # Automatically handles newlines\n",
    "\n",
    "    # Output the PDF to the file\n",
    "    pdf.output(output_pdf_path)\n",
    "\n",
    "# Loop through all PDF files in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.pdf'):  # Process only PDF files\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "\n",
    "        # Extract text from the PDF using the marker library\n",
    "        rendered = converter(file_path)\n",
    "        text, _, _ = text_from_rendered(rendered)\n",
    "\n",
    "        # Clean the extracted text using regular expressions\n",
    "        cleaned_text = clean_text(text)\n",
    "\n",
    "        # Define the output path for the cleaned PDF (save it with a \"cleaned_\" prefix)\n",
    "        output_pdf_path = os.path.join(output_directory, f\"cleaned_{filename}\")\n",
    "\n",
    "        # Save the cleaned text as a new PDF\n",
    "        save_text_as_pdf(cleaned_text, output_pdf_path)\n",
    "\n",
    "        print(f\"Processed and saved cleaned PDF: {output_pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "llm = Groq(model=\"llama3-70b-8192\", api_key='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.readers.file import PDFReader\n",
    "\n",
    "pdf_folder = '/content/output_pdfs'  # Path to your PDF folder\n",
    "pdf_reader_obj = PDFReader(return_full_document=True)\n",
    "\n",
    "# Load documents in a loop to handle multiple files\n",
    "documents = []\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.endswith('.pdf'):  # Process only PDF files\n",
    "        file_path = os.path.join(pdf_folder, filename)\n",
    "        documents.extend(pdf_reader_obj.load_data(file_path))  # Use extend to add documents to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(documents) = }\\n\")\n",
    "for doc in documents[:]:\n",
    "  print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatiate the text from pages (documents) into a single string\n",
    "full_text = \"\"\n",
    "for doc in documents:\n",
    "  full_text += doc.text + \"\\n\"\n",
    "\n",
    "print(full_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "text_parser = TokenTextSplitter(\n",
    "    chunk_size=128,\n",
    "    chunk_overlap=8\n",
    ")\n",
    "\n",
    "chunks = text_parser.split_text(text=full_text)\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert chunks into llama nodes\n",
    "nodes = []\n",
    "for chunk_text in chunks:\n",
    "  node = TextNode(text=chunk_text)\n",
    "  nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the embedding model from hugging face\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Create embeddings for the chunks\n",
    "for node in tqdm(nodes):\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Create a collection called \"manipal_docs\" in chromadb where our chunks\n",
    "# can be stored\n",
    "db = chromadb.EphemeralClient()\n",
    "chroma_collection = db.get_or_create_collection(\"MSISBDA\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes, storage_context=storage_context, embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "# Create a retriever object\n",
    "retriever = index.as_retriever(similarity_top_k=10)\n",
    "# OR\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "top_chunks = retriever.retrieve(\"What is the courses offered at BDA?\")\n",
    "print(len(top_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_chunks[0].text)\n",
    "print(top_chunks[1].text)\n",
    "print(top_chunks[2].text)\n",
    "print(top_chunks[3].text)\n",
    "print(top_chunks[4].text)\n",
    "print(top_chunks[5].text)\n",
    "print(top_chunks[6].text)\n",
    "print(top_chunks[7].text)\n",
    "print(top_chunks[8].text)\n",
    "print(top_chunks[9].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# Create prompt\n",
    "template = (\n",
    "        \"Context information is below.\\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"{context_str}\\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"Given the context information and not prior knowledge, \"\n",
    "        \"answer the query.\\n\"\n",
    "        \"Query: {query_str}\\n\"\n",
    "        \"Answer: \"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(llm, text_qa_template = qa_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.4)]\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"What are the courses names at BDA\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
